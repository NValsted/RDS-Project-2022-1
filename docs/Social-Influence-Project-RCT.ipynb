{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb9df05",
   "metadata": {},
   "source": [
    "# Social Influence Project - Reddit Submission Popularity RCT <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "\n",
    "This notebook concretely illustrates the data gathering and analysis for which the main paper is based upon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fbe8a2",
   "metadata": {},
   "source": [
    "# Table of contents:\n",
    "* [Data gathering setup and pipeline](#data-gather)\n",
    "* [Data analysis](#data-analysis)\n",
    "    * [Load data](#load-data)\n",
    "    * [Preprocessing](#preprocessing)\n",
    "    * [Distribution similarity](#dist-similarity)\n",
    "    * [Temporal development](#temporal-development)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759aabd",
   "metadata": {},
   "source": [
    "# Data gathering setup and pipeline <a class=\"anchor\" id=\"data-gather\"></a>\n",
    "\n",
    "The following section contains all the relevant python code used to interface with reddit and store the resulting data in a local SQLite database. Keep in mind that the code is not meant to be deployed and run from within a jupyter notebook. Instead, it was designed such that the entrypoint `main.py` can be run on a schedule with a cronjob. For a greater viewing experience and better overview, it is recommended to visit [the GitHub repo](https://github.com/NValsted/RDS-Project-2022-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf96405f",
   "metadata": {},
   "source": [
    "### Database interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48cdb0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#src/database.py\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, TypeVar, List, Type\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.sql.schema import Table\n",
    "from sqlmodel import create_engine, SQLModel, Session\n",
    "\n",
    "ModelType = TypeVar(\"ModelType\", bound=SQLModel)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Database:\n",
    "    \"\"\"\n",
    "    Database class with methods to create/drop tables and add/retrieve table entries\n",
    "    \"\"\"\n",
    "    engine: Engine\n",
    "\n",
    "    @contextmanager\n",
    "    def session(self):\n",
    "        with Session(self.engine) as session:\n",
    "            yield session\n",
    "\n",
    "    def create_tables(self, tables: Optional[List[Table]] = None) -> None:\n",
    "        SQLModel.metadata.create_all(self.engine, tables=tables)\n",
    "\n",
    "    def drop_tables(self, tables: Optional[List[Table]] = None) -> None:\n",
    "        SQLModel.metadata.drop_all(self.engine, tables=tables)\n",
    "\n",
    "    def add(self, instances: List[ModelType]) -> None:\n",
    "        with self.session() as session:\n",
    "            session.add_all(instances)\n",
    "            session.commit()\n",
    "\n",
    "    def get(self, model: Type[ModelType], id: int) -> Optional[ModelType]:\n",
    "        with Session(self.engine) as session:\n",
    "            matches = session.query(model).filter(model.id == id).all()\n",
    "            if len(matches) > 1:\n",
    "                raise ValueError(\n",
    "                    f\"Multiple matches for {id=} in {model.__name__}:\\n{matches}\"\n",
    "                )\n",
    "            elif len(matches) == 0:\n",
    "                return None\n",
    "            return matches[0]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DBFactory:\n",
    "    \"\"\"\n",
    "    Factory to create Database instances\n",
    "    \"\"\"\n",
    "    engine_url: str = \"sqlite:///../database.db\"\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Database:\n",
    "        engine = create_engine(url=self.engine_url, **kwargs)\n",
    "        return Database(engine=engine, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bdf539",
   "metadata": {},
   "source": [
    "### Model and database table definitions\n",
    "Python Pydantic models and SQLite table definitions are made simultaneously using the SQLModel ORM capabilities\n",
    "\n",
    "A `RedditPost` entry will be created once when a post is fetched the first time, which includes generic metadata about the post, while a `RedditPostLogPoint` will be created periodically, which is responsible for keeping track of the score and number of comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b630ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/database_models.py\n",
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "from sqlalchemy import Column, Enum as SAEnum\n",
    "from sqlmodel import SQLModel, Field\n",
    "\n",
    "\n",
    "class GroupEnum(str, Enum):\n",
    "    CONTROL = \"CONTROL\"\n",
    "    TREATMENT = \"TREATMENT\"\n",
    "\n",
    "\n",
    "class RedditPost(SQLModel):\n",
    "    id: str = Field(primary_key=True, index=True)\n",
    "    batch_id: str = Field(\n",
    "        index=True,\n",
    "        description=\"Unique ID of the batch in which the post was added\",\n",
    "    )\n",
    "    active: bool = Field(\n",
    "        default=True, description=\"Indicates whether the post is reachable\"\n",
    "    )\n",
    "    group: GroupEnum = Field(sa_column=Column(SAEnum(GroupEnum)))\n",
    "    subreddit: str = Field()\n",
    "    title: str = Field()\n",
    "    creation_date: datetime = Field(description=\"Date at which post was created\")\n",
    "\n",
    "\n",
    "class RedditPostTable(RedditPost, table=True):\n",
    "    __tablename__ = \"RedditPost\"\n",
    "\n",
    "\n",
    "class RedditPostLogPoint(SQLModel):\n",
    "    pk: Optional[int] = Field(primary_key=True, default=None, index=True)\n",
    "    id: str = Field(index=True)\n",
    "    score: int = Field()\n",
    "    num_comments: int = Field()\n",
    "    date: datetime = Field(description=\"Date at which stats were collected\")\n",
    "\n",
    "\n",
    "class RedditPostLogPointTable(RedditPostLogPoint, table=True):\n",
    "    __tablename__ = \"RedditPostLogPoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed8ba41",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "A few utility functions are also defined which are primarily concerned with increasing the robustness of the data gathering solution.\n",
    "\n",
    "The logger and its factory method provide a structured interface for saving logs persistently to disk when the main job runs for longer periods of time without manual intervention, and the `safe_call` will prevent the process from terminating immediately on any error - e.g. if a single post out of thousands in a batch raise a 404 error because it was deleted, then we still want to process the rest of the batch. Likewise, it makes sense to retry requests if the connection temporarily drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c2a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/utils.py\n",
    "from typing import Callable, Any, List, Dict, Optional\n",
    "from time import sleep\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "from prawcore import exceptions\n",
    "\n",
    "\n",
    "def get_logger(name: str = \"RDS-PROJECT\") -> logging.Logger:\n",
    "    logger = logging.getLogger(name)\n",
    "    fhandler = logging.FileHandler(filename=\"logs.log\", mode=\"a\")\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    fhandler.setFormatter(formatter)\n",
    "    logger.addHandler(fhandler)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def safe_call(\n",
    "    func: Callable,\n",
    "    args: Optional[List] = None,\n",
    "    kwargs: Optional[Dict] = None,\n",
    "    max_retries: int = 3,\n",
    "    sleep_time: int = 1,\n",
    "    exception: Exception = exceptions.NotFound,\n",
    "    raise_on_failure: bool = True,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Wraps a function and retries it if it raises an exception.\n",
    "    \"\"\"\n",
    "    logger = get_logger()\n",
    "\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    error = Exception(\"Unknown error\")\n",
    "\n",
    "    while max_retries > 0:\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except exception as e:\n",
    "            error = e\n",
    "            max_retries -= 1\n",
    "            logger.info(\n",
    "                f\"{func.__name__} failed with args {args} and kwargs {kwargs}\\n\"\n",
    "                f\"{e}\\n{traceback.format_exc()}\"\n",
    "                f\"{max_retries} retries left\"\n",
    "            )\n",
    "            sleep(sleep_time)\n",
    "\n",
    "    if raise_on_failure:\n",
    "        logger.error(f\"Failed to execute function {func.__name__}\")\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e0f73",
   "metadata": {},
   "source": [
    "### The reddit interface\n",
    "Interfacing with reddit is done via the praw reddit API wrapper, which in turn is wrapped in the RedditBot class. This class provides methods to:\n",
    "- Fetch new posts:\n",
    "    - A random batch of new posts can be fetched with `get_batch_of_posts`\n",
    "    - This batch of posts can be randomly divided into CONTROL and TREATMENT groups with `group_posts`\n",
    "- Interface with database:\n",
    "    - Add:\n",
    "        - Posts (i.e. metadata about subreddit, creation_date, CONTROL/TREATMENT group, etc.) can be added with `add_posts_to_db`\n",
    "        - Log points (i.e. observations of score and number of comments) can be added with `add_log_points`\n",
    "    - Get:\n",
    "        - Posts no older than a certain amount of days can be retrieved from the database with `get_stored_posts`\n",
    "- The data (title, score, etc.) for a list of posts can be fetched given a list of ids with `get_posts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a39c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/reddit_bot.py\n",
    "import os\n",
    "import random\n",
    "import traceback\n",
    "from datetime import datetime, timedelta\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from uuid import uuid4\n",
    "import json\n",
    "\n",
    "import praw\n",
    "\n",
    "# Local imports suppressed in notebook cells since the objects are already available in scope.\n",
    "# from src.database import DBFactory\n",
    "# from src.database_models import RedditPostTable, RedditPostLogPointTable, GroupEnum\n",
    "# from src.utils import get_logger, safe_call\n",
    "\n",
    "CLIENT_ID = os.getenv(\"CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"CLIENT_SECRET\")\n",
    "USERNAME = os.getenv(\"USERNAME\")\n",
    "PASSWORD = os.getenv(\"PASSWORD\")\n",
    "USER_AGENT = os.getenv(\"USER_AGENT\")\n",
    "RATELIMIT = int(os.getenv(\"RATELIMIT\", 5))\n",
    "\n",
    "logger = get_logger(\"REDDIT-BOT\")\n",
    "\n",
    "\n",
    "class RedditBot:\n",
    "    \"\"\"\n",
    "    Wrapper for the Reddit bot.\n",
    "\n",
    "    It contains the following methods:\n",
    "    - get_batch_of_posts: Selects a batch of posts for the experiment\n",
    "    - group_posts: Groups a list of posts into treatment and control groups\n",
    "    - add_posts_to_db: Adds a list of posts to the database\n",
    "    - add_log_points: Adds a list of log points to the database\n",
    "    - get_stored_posts: Fetches posts from the database given a date filter\n",
    "    - get_posts: Fetches posts from the Reddit API given a list of ids\n",
    "    \"\"\"\n",
    "\n",
    "    reddit: praw.Reddit\n",
    "    url: str = \"https://www.reddit.com\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        client_id: str = CLIENT_ID,\n",
    "        client_secret: str = CLIENT_SECRET,\n",
    "        username: str = USERNAME,\n",
    "        password: str = PASSWORD,\n",
    "        user_agent: str = USER_AGENT,\n",
    "        ratelimit: int = RATELIMIT,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Authenticates the bot and initializes the Reddit instance.\n",
    "        \"\"\"\n",
    "        assert isinstance(client_id, str)\n",
    "        assert isinstance(client_secret, str)\n",
    "        assert isinstance(username, str)\n",
    "        assert isinstance(password, str)\n",
    "        assert isinstance(user_agent, str)\n",
    "        assert isinstance(ratelimit, int)\n",
    "\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret,\n",
    "            username=username,\n",
    "            password=password,\n",
    "            user_agent=user_agent,\n",
    "            ratelimit=ratelimit,\n",
    "        )\n",
    "\n",
    "    def get_batch_of_posts(\n",
    "        self,\n",
    "        subreddit: str = \"all\",\n",
    "        score: int = 1,\n",
    "        num_comments: int = 1,\n",
    "        batch_size: int = 64,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Selects a batch of posts with at most 'score' number of upvotes and\n",
    "        'num_comments' number of comments in the given subreddit.\n",
    "\n",
    "        NOTE: batch_size is an upper bound on the number of posts returned.\n",
    "        \"\"\"\n",
    "\n",
    "        posts = [\n",
    "            post\n",
    "            for post in self.reddit.subreddit(subreddit).new(limit=batch_size)\n",
    "            if post.score <= score and post.num_comments <= num_comments\n",
    "        ]\n",
    "\n",
    "        return posts\n",
    "\n",
    "    @staticmethod\n",
    "    def group_posts(\n",
    "        posts: List[praw.models.Submission],\n",
    "    ) -> Tuple[List[praw.models.Submission], List[praw.models.Submission]]:\n",
    "        \"\"\"\n",
    "        Assigns posts into treatment and control groups.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_id = str(uuid4())\n",
    "        random.shuffle(posts)\n",
    "        if len(posts) % 2 != 0:\n",
    "            posts.pop()  # Drop a random post to make the list even\n",
    "\n",
    "        middle = len(posts) // 2\n",
    "        treatment_posts = posts[:middle]\n",
    "        control_posts = posts[middle:]\n",
    "\n",
    "        for post in treatment_posts:\n",
    "            post.upvote()\n",
    "            post.group = GroupEnum.TREATMENT\n",
    "            post.batch_id = batch_id\n",
    "\n",
    "        for post in control_posts:\n",
    "            post.group = GroupEnum.CONTROL\n",
    "            post.batch_id = batch_id\n",
    "\n",
    "        return treatment_posts, control_posts\n",
    "\n",
    "    @staticmethod\n",
    "    def add_posts_to_db(\n",
    "        posts: List[praw.models.Submission],\n",
    "        backup: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "        prepared_posts = []\n",
    "\n",
    "        def _prepare_post(post: praw.models.Submission) -> Dict:\n",
    "            return dict(\n",
    "                id=post.id,\n",
    "                batch_id=post.batch_id,\n",
    "                group=post.group,\n",
    "                subreddit=str(post.subreddit),\n",
    "                title=post.title,\n",
    "                creation_date=post.created_utc,\n",
    "            )\n",
    "\n",
    "        for post in posts:\n",
    "            prepared_post = safe_call(\n",
    "                _prepare_post,\n",
    "                args=[post],\n",
    "                exception=Exception,\n",
    "                raise_on_failure=False,\n",
    "            )\n",
    "            if prepared_post is not None:\n",
    "                prepared_posts.append(prepared_post)\n",
    "\n",
    "        if backup:\n",
    "            today = datetime.today().date().isoformat()\n",
    "            with open(f\"backup/REDDITBOT_{today}_{str(uuid4())}.json\", \"w\") as f:\n",
    "                json.dump(prepared_posts, f, indent=4, default=str)\n",
    "\n",
    "        db = DBFactory()()\n",
    "\n",
    "        parsed_posts = [RedditPostTable(**post) for post in prepared_posts]\n",
    "        db.add(parsed_posts)\n",
    "        logger.info(f\"Added {len(parsed_posts)} posts to the database\")\n",
    "\n",
    "        RedditBot.add_log_points(posts, backup=backup)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_log_points(\n",
    "        posts: List[praw.models.Submission], backup: bool = False\n",
    "    ) -> None:\n",
    "\n",
    "        prepared_posts = []\n",
    "        stale_posts = []\n",
    "\n",
    "        def _prepare_post(post: praw.models.Submission) -> Dict:\n",
    "            return dict(\n",
    "                id=post.id,\n",
    "                score=post.score,\n",
    "                num_comments=post.num_comments,\n",
    "                date=datetime.now(),\n",
    "            )\n",
    "\n",
    "        for post in posts:\n",
    "            prepared_post = safe_call(\n",
    "                _prepare_post,\n",
    "                args=[post],\n",
    "                exception=Exception,\n",
    "                raise_on_failure=False,\n",
    "            )\n",
    "            if prepared_post is not None:\n",
    "                prepared_posts.append(prepared_post)\n",
    "            else:\n",
    "                stale_posts.append(post)\n",
    "\n",
    "        if backup:\n",
    "            today = datetime.today().date().isoformat()\n",
    "            with open(f\"backup/REDDITBOT_{today}_{str(uuid4())}.json\", \"w\") as f:\n",
    "                json.dump(prepared_posts, f, indent=4, default=str)\n",
    "\n",
    "        db = DBFactory()()\n",
    "\n",
    "        parsed_posts = [RedditPostLogPointTable(**post) for post in prepared_posts]\n",
    "        db.add(parsed_posts)\n",
    "        logger.info(f\"Added {len(parsed_posts)} log points to database\")\n",
    "\n",
    "        for post in stale_posts:\n",
    "            old_instance = db.get(RedditPostTable, id=post.id)\n",
    "            if old_instance is not None:\n",
    "                old_instance.active = False\n",
    "                db.add([old_instance])\n",
    "\n",
    "        logger.info(f\"Marked {len(stale_posts)} stale posts\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_stored_posts(max_age: int = 8) -> List[RedditPostTable]:\n",
    "        db = DBFactory()()\n",
    "\n",
    "        with db.session() as session:\n",
    "            posts = (\n",
    "                session.query(RedditPostTable)\n",
    "                .filter(\n",
    "                    RedditPostTable.creation_date\n",
    "                    >= (datetime.now() - timedelta(days=max_age))\n",
    "                )\n",
    "                .filter(RedditPostTable.active)\n",
    "                .all()\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Fetched {len(posts)} active posts from the database\")\n",
    "\n",
    "            return posts\n",
    "\n",
    "    def _submission_wrapper(self, *args, **kwargs) -> Optional[praw.models.Submission]:\n",
    "        try:\n",
    "            return self.reddit.submission(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                f\"{e}\\n{traceback.format_exc()}\\nargs: {args}\\nkwargs: {kwargs}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    def get_posts(\n",
    "        self, ids: List[str], threads: int = 4\n",
    "    ) -> List[praw.models.Submission]:\n",
    "        with ThreadPool(threads) as pool:\n",
    "            posts = pool.map(self._submission_wrapper, ids)\n",
    "\n",
    "        posts = [post for post in posts if post is not None]\n",
    "\n",
    "        return posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3d40ed",
   "metadata": {},
   "source": [
    "### One-time entrypoint - `setup.py`\n",
    "The `setup` function simply establishes a connection to- and possibly creates the database, after which it drops any existing tables and creates new ones afresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32395a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup.py\n",
    "\n",
    "# Local imports suppressed in notebook cells since the objects are already available in scope.\n",
    "# from src.database_models import RedditPostTable, RedditPostLogPointTable  # NOQA : F401\n",
    "# from src.database import DBFactory\n",
    "\n",
    "\n",
    "def setup():\n",
    "    db = DBFactory()()\n",
    "    db.drop_tables()\n",
    "    db.create_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f43d3e",
   "metadata": {},
   "source": [
    "### Main entrypoint - `main.py`\n",
    "With everything set up, the `main` function defines a simple routine for monitoring the stats of previously fetched posts as well as fetching a batch of newly created posts. This is the function that is deployed to run periodically. Check [the GitHub repo](https://github.com/NValsted/RDS-Project-2022-1) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af34d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "# Local imports suppressed in notebook cells since the objects are already available in scope.\n",
    "# from src import RedditBot\n",
    "# from src.utils import safe_call\n",
    "\n",
    "\n",
    "def main():\n",
    "    bot = RedditBot()\n",
    "\n",
    "    # Update old Posts\n",
    "    posts = bot.get_stored_posts()\n",
    "    ids = {post.id for post in posts}\n",
    "\n",
    "    posts = bot.get_posts(ids)\n",
    "    bot.add_log_points(posts)\n",
    "\n",
    "    # New posts\n",
    "    treatment, control = safe_call(\n",
    "        func=lambda: bot.group_posts(bot.get_batch_of_posts())\n",
    "    )\n",
    "    bot.add_posts_to_db(treatment)\n",
    "    bot.add_posts_to_db(control)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27102d",
   "metadata": {},
   "source": [
    "# Data analysis <a class=\"anchor\" id=\"data-analysis\"></a>\n",
    "At this point, roughly 45000 posts have been fetched and monitored over the course of 7 days each, which has resulted in a little under 3 million log points. This section marks the start of an analysis of the resulting data.\n",
    "\n",
    "A snapshot of the database is available at https://ituniversity-my.sharepoint.com/:u:/g/personal/nicv_itu_dk/EU18k8csh_FOvCMkORzrVL0BNiXTQbgaj7rKu5rGawACxA?e=miF6F4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f647ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df1c96",
   "metadata": {},
   "source": [
    "## Load data <a class=\"anchor\" id=\"load-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d19ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_post_df = pd.read_sql_table(RedditPostTable.__tablename__, DBFactory.engine_url)\n",
    "reddit_post_log_point_df = pd.read_sql_table(RedditPostLogPointTable.__tablename__, DBFactory.engine_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94eecc",
   "metadata": {},
   "source": [
    "## Preprocessing <a class=\"anchor\" id=\"preprocessing\"></a>\n",
    "\n",
    "Before commencing with the analysis, a little preprocessing is beneficial, e.g. due to the fact that certain posts are marked inactive since they have been deleted or otherwise made unreachable, which has unbalanced the dataset slightly.\n",
    "\n",
    "The preprocessing steps are the following (Which are intertwined in practice for convenience):\n",
    "- Join post info and log points (`RedditPostTable` and `RedditPostLogPointTable`)\n",
    "- Balance dataset. \n",
    "    - For each inactive post, identify a post from the conjugate group with the same `batch_id` and filter away both.\n",
    "    - Filter away any post that has not been monitored for at least 7 days (i.e. younger than 7 days or marked inactive before the 7 day mark).\n",
    "- Create derived columns: `age` and `saturation`.\n",
    "- Unbias treatment group by subtracting 1 from the score\n",
    "- For each of the two groups, create dataframes containing only the latest log point for a post.\n",
    "\n",
    "These steps will be described further in the relevant cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f326aeda",
   "metadata": {},
   "source": [
    "### Join dataframes\n",
    "A join between the RedditPost and RedditPostLogPoint on the `id` column is done (reddit's id for a given post), which essentially yields RedditPostLogPoint but with all the relevant metadata attached for the post which the log point corresponds to. A random sample of the dataframe is shown as an example of the resulting format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d6fea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = pd.merge(reddit_post_df, reddit_post_log_point_df, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96e67e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>id</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>active</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>pk</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>722459</th>\n",
       "      <td>TREATMENT</td>\n",
       "      <td>tgxypn</td>\n",
       "      <td>988c888f-bbd8-4eb7-9214-605c3d4a1951</td>\n",
       "      <td>True</td>\n",
       "      <td>tylerthecreator</td>\n",
       "      <td>LUMBERJACK Animation (alongside others)</td>\n",
       "      <td>2022-03-18 10:06:32</td>\n",
       "      <td>701797</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-21 16:02:36.110044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491678</th>\n",
       "      <td>TREATMENT</td>\n",
       "      <td>tbkrjs</td>\n",
       "      <td>01750c94-7be3-4c86-9871-4c375c5c6e07</td>\n",
       "      <td>True</td>\n",
       "      <td>eating_disorders</td>\n",
       "      <td>Im scared of food</td>\n",
       "      <td>2022-03-11 07:33:05</td>\n",
       "      <td>532230</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-16 13:02:06.384619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453250</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>tamzf2</td>\n",
       "      <td>cb3f8a79-ba84-4cb1-9dfa-78c8dab39e10</td>\n",
       "      <td>True</td>\n",
       "      <td>elderscrollsonline</td>\n",
       "      <td>Stupid question</td>\n",
       "      <td>2022-03-10 01:23:04</td>\n",
       "      <td>230924</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-03-10 03:38:03.023124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909648</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>uaxzv4</td>\n",
       "      <td>1b76ffe1-fe16-41de-b4c3-a3c1dba104cc</td>\n",
       "      <td>True</td>\n",
       "      <td>RPClipsGTA</td>\n",
       "      <td>Flippy a changed man</td>\n",
       "      <td>2022-04-24 16:06:15</td>\n",
       "      <td>1976233</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-04-30 18:35:18.337006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478062</th>\n",
       "      <td>TREATMENT</td>\n",
       "      <td>u11qgv</td>\n",
       "      <td>f2a86026-e600-4d76-8de6-f872aa3ff0c5</td>\n",
       "      <td>True</td>\n",
       "      <td>footballmanagergames</td>\n",
       "      <td>custom databases</td>\n",
       "      <td>2022-04-11 07:04:57</td>\n",
       "      <td>1466168</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-04-14 21:09:52.165371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             group      id                              batch_id  active  \\\n",
       "722459   TREATMENT  tgxypn  988c888f-bbd8-4eb7-9214-605c3d4a1951    True   \n",
       "491678   TREATMENT  tbkrjs  01750c94-7be3-4c86-9871-4c375c5c6e07    True   \n",
       "453250     CONTROL  tamzf2  cb3f8a79-ba84-4cb1-9dfa-78c8dab39e10    True   \n",
       "1909648    CONTROL  uaxzv4  1b76ffe1-fe16-41de-b4c3-a3c1dba104cc    True   \n",
       "1478062  TREATMENT  u11qgv  f2a86026-e600-4d76-8de6-f872aa3ff0c5    True   \n",
       "\n",
       "                    subreddit                                    title  \\\n",
       "722459        tylerthecreator  LUMBERJACK Animation (alongside others)   \n",
       "491678       eating_disorders                        Im scared of food   \n",
       "453250     elderscrollsonline                          Stupid question   \n",
       "1909648            RPClipsGTA                     Flippy a changed man   \n",
       "1478062  footballmanagergames                         custom databases   \n",
       "\n",
       "              creation_date       pk  score  num_comments  \\\n",
       "722459  2022-03-18 10:06:32   701797      3             0   \n",
       "491678  2022-03-11 07:33:05   532230      2             0   \n",
       "453250  2022-03-10 01:23:04   230924      0             5   \n",
       "1909648 2022-04-24 16:06:15  1976233      0             1   \n",
       "1478062 2022-04-11 07:04:57  1466168      2             0   \n",
       "\n",
       "                              date  \n",
       "722459  2022-03-21 16:02:36.110044  \n",
       "491678  2022-03-16 13:02:06.384619  \n",
       "453250  2022-03-10 03:38:03.023124  \n",
       "1909648 2022-04-30 18:35:18.337006  \n",
       "1478062 2022-04-14 21:09:52.165371  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b9e7db",
   "metadata": {},
   "source": [
    "### Derived columns p0 - age\n",
    "The `age` column simply denotes how long it has been since a post was created when a log point was recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7bb0106",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined[\"age\"] = joined[\"date\"] - joined[\"creation_date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55acf5",
   "metadata": {},
   "source": [
    "### Unbias TREATMENT group\n",
    "The initial upvote of 1 is subtracted from the TREATMENT group, since we are interested in investigating whether the treatment has increased popularity as expressed by external users - i.e. all users that are not our bot.\n",
    "\n",
    "Perhaps, this is best explained with an example in the extremes: If we assume an artificial world where no other users interact with posts on the platform, then we can safely say beforehand that the treatment will not have an effect. If we then perform our experiment, all posts in the CONTROL group will have a score of 0, and all posts in the TREATMENT group will have a score of 1. If we were to perform any analysis on the resulting data without unbiasing the TREATMENT group, we would wrongfully conclude effectiveness of the TREATMENT due to the difference in score distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdc68433",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.loc[joined[\"group\"] == GroupEnum.TREATMENT.value, (\"score\")] -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d0a1b4",
   "metadata": {},
   "source": [
    "### Latest posts\n",
    "We have record many log points for each individual post, so the `joined_latest` dataframe consists of only the latest log point for each post  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "484b3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_latest = joined.sort_values([\"date\"], ascending=False).groupby(by=\"id\", as_index=False).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f36d7d",
   "metadata": {},
   "source": [
    "### Balance dataset\n",
    "To balance the dataset, we want remove any posts that have not yet been tracked for at least 7 days, and we also want to ensure that there are equally many data points in the control and treatment groups. Since we sample an equal amount of control and treatment posts in each batch, the only reason these can be different is if one or more posts have been marked inactive (i.e. deleted or otherwise unreachable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "034c6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_drop = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa300b",
   "metadata": {},
   "source": [
    "First, filter posts younger than 7 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6443046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in joined_latest[joined_latest[\"age\"] < timedelta(days=7)].iterrows():\n",
    "    ids_to_drop.add(str(row[\"id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6644a7f",
   "metadata": {},
   "source": [
    "Then balance pairs within batches, prioritizing removing conjugate posts which are also inactive - e.g. if a batch contains a single inactive control post and inactive treatment post, then these two should simply cancel out. Otherwise, simply sample a random active post of the conjugate group to cancel out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf365747",
   "metadata": {},
   "outputs": [],
   "source": [
    "inactive_posts = joined_latest[joined_latest[\"active\"] == False]\n",
    "\n",
    "for group in inactive_posts.groupby(by=\"batch_id\", as_index=False):\n",
    "    group_key, group_df = group\n",
    "\n",
    "    control_remainder, treatment_remainder = (\n",
    "        joined_latest[\n",
    "            (joined_latest[\"batch_id\"] == group_key)\n",
    "            & (joined_latest[\"group\"] == group.value)\n",
    "            & ~(joined_latest[\"id\"].isin(set(group_df[\"id\"])))\n",
    "        ]\n",
    "        for group in (GroupEnum.CONTROL, GroupEnum.TREATMENT)\n",
    "    )\n",
    "    num_control = control_remainder.shape[0]\n",
    "    num_treatment = treatment_remainder.shape[0]\n",
    "    \n",
    "    if num_control != num_treatment:\n",
    "        conjugate_remainder = control_remainder if num_treatment < num_control else treatment_remainder\n",
    "        num_to_drop = abs(num_control - num_treatment)\n",
    "        to_drop = conjugate_remainder.sample(n=num_to_drop)\n",
    "        ids_to_drop.add(str(to_drop[\"id\"]))\n",
    "        \n",
    "    for entry in group_df[\"id\"]:\n",
    "        ids_to_drop.add(str(entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1663f278",
   "metadata": {},
   "source": [
    "Apply filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "869a228f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 4588 post(s)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dropping {len(ids_to_drop)} post(s)\")\n",
    "\n",
    "joined_latest = joined_latest[~(joined_latest[\"id\"].isin(ids_to_drop))]\n",
    "# joined = joined[~(joined[\"id\"].isin(ids_to_drop))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1ecf4",
   "metadata": {},
   "source": [
    "### Derived columns p1 - saturation\n",
    "Saturation is useful in a temporal context and describes the ratio of some maximum value for a post. E.g. a post with `score` values 0, 50, and 100 will be mapped to `score_saturation` values of 0, 0.5, 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ac30f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined[\"score_saturation\"] = joined[\"score\"] / joined.groupby(\"id\")[\"score\"].transform(np.max)\n",
    "joined[\"num_comments_saturation\"] = joined[\"num_comments\"] / joined.groupby(\"id\")[\"num_comments\"].transform(np.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ede8aa",
   "metadata": {},
   "source": [
    "### Overview of resulting dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4771b2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>id</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>active</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>pk</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>date</th>\n",
       "      <th>age</th>\n",
       "      <th>score_saturation</th>\n",
       "      <th>num_comments_saturation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46930</th>\n",
       "      <td>TREATMENT</td>\n",
       "      <td>t7f4wj</td>\n",
       "      <td>42bc7b6d-4573-49da-8edd-eb3b683292f0</td>\n",
       "      <td>True</td>\n",
       "      <td>Sims3</td>\n",
       "      <td>Monte Vista appreciation post</td>\n",
       "      <td>2022-03-05 18:05:59</td>\n",
       "      <td>12826</td>\n",
       "      <td>171</td>\n",
       "      <td>7</td>\n",
       "      <td>2022-03-06 08:05:28.511898</td>\n",
       "      <td>0 days 13:59:29.511898</td>\n",
       "      <td>0.806604</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748012</th>\n",
       "      <td>TREATMENT</td>\n",
       "      <td>thn7au</td>\n",
       "      <td>77a0c34a-3d51-4000-a685-7fc8fee72014</td>\n",
       "      <td>True</td>\n",
       "      <td>skyrim</td>\n",
       "      <td>My newest character</td>\n",
       "      <td>2022-03-19 04:06:41</td>\n",
       "      <td>817871</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-03-25 06:30:38.239367</td>\n",
       "      <td>6 days 02:23:57.239367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769618</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>uv35qn</td>\n",
       "      <td>e57097bc-c75f-4352-b095-17ce6b191feb</td>\n",
       "      <td>True</td>\n",
       "      <td>cricut</td>\n",
       "      <td>Already on my second engraving project. If you...</td>\n",
       "      <td>2022-05-22 04:05:14</td>\n",
       "      <td>2686070</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "      <td>2022-05-23 15:01:32.448292</td>\n",
       "      <td>1 days 10:56:18.448292</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640449</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>texoz0</td>\n",
       "      <td>17c8e68b-8e89-4220-ae15-1a8cce7360d4</td>\n",
       "      <td>True</td>\n",
       "      <td>Purpose</td>\n",
       "      <td>How To Improve Your Work-Life Balance - Startu...</td>\n",
       "      <td>2022-03-15 19:26:04</td>\n",
       "      <td>552206</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-17 03:28:50.193380</td>\n",
       "      <td>1 days 08:02:46.193380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2176804</th>\n",
       "      <td>TREATMENT</td>\n",
       "      <td>uhadet</td>\n",
       "      <td>82670c87-92b0-4795-a329-8bb9d01d1783</td>\n",
       "      <td>True</td>\n",
       "      <td>livingaparttogether</td>\n",
       "      <td>Thinking of doing this but have qs</td>\n",
       "      <td>2022-05-03 07:03:54</td>\n",
       "      <td>2233895</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-09 00:36:05.150412</td>\n",
       "      <td>5 days 17:32:11.150412</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             group      id                              batch_id  active  \\\n",
       "46930    TREATMENT  t7f4wj  42bc7b6d-4573-49da-8edd-eb3b683292f0    True   \n",
       "748012   TREATMENT  thn7au  77a0c34a-3d51-4000-a685-7fc8fee72014    True   \n",
       "2769618    CONTROL  uv35qn  e57097bc-c75f-4352-b095-17ce6b191feb    True   \n",
       "640449     CONTROL  texoz0  17c8e68b-8e89-4220-ae15-1a8cce7360d4    True   \n",
       "2176804  TREATMENT  uhadet  82670c87-92b0-4795-a329-8bb9d01d1783    True   \n",
       "\n",
       "                   subreddit  \\\n",
       "46930                  Sims3   \n",
       "748012                skyrim   \n",
       "2769618               cricut   \n",
       "640449               Purpose   \n",
       "2176804  livingaparttogether   \n",
       "\n",
       "                                                     title  \\\n",
       "46930                        Monte Vista appreciation post   \n",
       "748012                                 My newest character   \n",
       "2769618  Already on my second engraving project. If you...   \n",
       "640449   How To Improve Your Work-Life Balance - Startu...   \n",
       "2176804                 Thinking of doing this but have qs   \n",
       "\n",
       "              creation_date       pk  score  num_comments  \\\n",
       "46930   2022-03-05 18:05:59    12826    171             7   \n",
       "748012  2022-03-19 04:06:41   817871      0             1   \n",
       "2769618 2022-05-22 04:05:14  2686070     31            20   \n",
       "640449  2022-03-15 19:26:04   552206      2             0   \n",
       "2176804 2022-05-03 07:03:54  2233895      4             3   \n",
       "\n",
       "                              date                    age  score_saturation  \\\n",
       "46930   2022-03-06 08:05:28.511898 0 days 13:59:29.511898          0.806604   \n",
       "748012  2022-03-25 06:30:38.239367 6 days 02:23:57.239367          0.000000   \n",
       "2769618 2022-05-23 15:01:32.448292 1 days 10:56:18.448292          0.885714   \n",
       "640449  2022-03-17 03:28:50.193380 1 days 08:02:46.193380          1.000000   \n",
       "2176804 2022-05-09 00:36:05.150412 5 days 17:32:11.150412          0.800000   \n",
       "\n",
       "         num_comments_saturation  \n",
       "46930                   0.538462  \n",
       "748012                  1.000000  \n",
       "2769618                 1.000000  \n",
       "640449                       NaN  \n",
       "2176804                 1.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38ac38",
   "metadata": {},
   "source": [
    "### Split dataframe in groups\n",
    "Finally, we simply create dataframes containing only values from the respective groups for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e906a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "control, treatment = (joined[joined[\"group\"] == group.value] for group in (GroupEnum.CONTROL, GroupEnum.TREATMENT))\n",
    "control_latest, treatment_latest = (\n",
    "    joined_latest[joined_latest[\"group\"] == group.value] for group in (GroupEnum.CONTROL, GroupEnum.TREATMENT)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0545e336",
   "metadata": {},
   "source": [
    "## Distribution similarity <a class=\"anchor\" id=\"dist-similarity\"></a>\n",
    "The following section investigates the similarity between the score and number of comments distributions between the two groups, in order to evaluate the effectiveness of the treatment.\n",
    "\n",
    "The following plot displays histograms of the data points within each group. The general distribution type seems similar, but parameters seem to be noticeably different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86a2c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import io as pio, graph_objects as go, express as px\n",
    "from plotly.subplots import make_subplots\n",
    "pio.renderers.default = \"iframe\"\n",
    "# Jupyter notebooks don't handle plotly figures well.\n",
    "# Therefore, iframes and %%capture are used to save the resulting html files to disk instead.\n",
    "# These should appear in the iframe_figures directory, following the figure_{cell_number}.html naming scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56afc4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=3,\n",
    "    cols=2,\n",
    "    shared_yaxes=True,\n",
    "    subplot_titles=(\"Score distribution\", \"Number of comments distribution\"),\n",
    "    horizontal_spacing=0.05,\n",
    ")\n",
    "\n",
    "for i in (1, 2, 3):\n",
    "    for j, attr in ((1, \"score\"), (2, \"num_comments\")):\n",
    "        for df, color in ((control_latest, \"#4969AA\"), (treatment_latest, \"#C90D0D\")):\n",
    "            if i == 3:\n",
    "                list_vals = df[attr].values.tolist()\n",
    "                num_bins = max(list_vals) - min(list_vals)\n",
    "                hist, bin_edges = np.histogram(list_vals, bins=num_bins, density=True)\n",
    "                cdf = np.cumsum(hist*np.diff(bin_edges))\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=bin_edges,\n",
    "                        y=np.concatenate(([0], cdf)),\n",
    "                        marker_color=color,\n",
    "                        showlegend=False,\n",
    "                    ),\n",
    "                    row=i,\n",
    "                    col=j\n",
    "                )\n",
    "            else:\n",
    "                fig.add_trace(\n",
    "                    go.Histogram(\n",
    "                        x=df[attr],\n",
    "                        histnorm=\"percent\",\n",
    "                        name=df[\"group\"].min(),\n",
    "                        xbins=dict(\n",
    "                            start=df[attr].min(),\n",
    "                            end=df[attr].max(),\n",
    "                            size=0.5\n",
    "                        ),\n",
    "                        marker_color=color,\n",
    "                        opacity=0.75,\n",
    "                        showlegend=i == 1 and j == 1\n",
    "                    ),\n",
    "                    row=i,\n",
    "                    col=j,\n",
    "                )\n",
    "\n",
    "                fig.update_layout(\n",
    "                    yaxis_title_text=\"Percent\",\n",
    "                    bargap=0.2,\n",
    "                    bargroupgap=0.1\n",
    "                )\n",
    "\n",
    "max_score = max(max(control_latest[\"score\"]), max(treatment_latest[\"score\"]))\n",
    "max_num_comments = max(max(control_latest[\"num_comments\"]), max(treatment_latest[\"num_comments\"]))\n",
    "                \n",
    "fig.update_xaxes(type=\"log\", row=1, col=1)\n",
    "fig.update_xaxes(type=\"log\", row=1, col=2)\n",
    "fig.update_xaxes(type=\"log\", row=2, col=1)\n",
    "fig.update_xaxes(type=\"log\", row=2, col=2)\n",
    "fig.update_xaxes(title_text=\"Score (log)\", row=3, col=1, type=\"log\", range=[np.log(-1), np.log(max_score)])\n",
    "fig.update_xaxes(title_text=\"Number of comments (log)\", row=3, col=2, type=\"log\", range=[np.log(0), np.log(max_num_comments)])\n",
    "\n",
    "fig.update_yaxes(title_text=\"Percent (log)\", type=\"log\", row=2, col=1)\n",
    "fig.update_yaxes(type=\"log\", row=2, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"CDF\", row=3, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b615d28c",
   "metadata": {},
   "source": [
    "### Kolmogorov-Smirnov test\n",
    "With the Kolmogorov-Smirnov test, we perform an empirical test to investigate whether the data points appear to belong to the same distribution. The null hypothesis is that the distributions are identical, and thus a low $p$-value indicates that there is a significant difference between the control and treatment groups. As can be seen below, the results are in favour of the alternative hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00dafe30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KstestResult(statistic=0.0535653193881042, pvalue=6.44996960612451e-26)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.kstest(control_latest[\"score\"], treatment_latest[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "261bf17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KstestResult(statistic=0.03160221636655669, pvalue=2.6799854846279296e-09)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.kstest(control_latest[\"num_comments\"], treatment_latest[\"num_comments\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc95ae8",
   "metadata": {},
   "source": [
    "However, small fluctuations might provide an inaccurate picture - e.g. a post with a score of 34 is probably not significantly different from one with a score of 35. So we will investigate whether rounding the score and number of comments to the nearest multiple of a range of numbers will jeopardize the result above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cc1c7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 21.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 19.26it/s]\n"
     ]
    }
   ],
   "source": [
    "binned_ks_test_score = [stats.kstest(*(df[\"score\"].map(lambda x: i * round(x / i)) for df in (control_latest, treatment_latest))).pvalue for i in tqdm(range(1, 10))]\n",
    "binned_ks_test_num_comments = [stats.kstest(*(df[\"num_comments\"].map(lambda x: i * round(x / i)) for df in (control_latest, treatment_latest))).pvalue for i in tqdm(range(1, 10))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c839c",
   "metadata": {},
   "source": [
    "...But as the plot below shows, even pessimistically trying several different bin sizes, the maximum $p$-value is still no larger than on the order of $10^{-6}$, which occurs for the score distributions when rounding the score to the nearest multiple of 5. Thus, it seems more likely that there is a significant difference between the control and treatment groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dcc671c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_26.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    subplot_titles=(\"K-S test binned score distributions\", \"KS-test binned number of comments distributions\"),\n",
    ")\n",
    "for i, lst in enumerate((binned_ks_test_score, binned_ks_test_num_comments)):\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, len(lst) + 1)), y=lst), row=i+1, col=1)\n",
    "fig.update_yaxes(type=\"log\", row=\"all\", col=\"all\")\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1da39c",
   "metadata": {},
   "source": [
    "Despite these results, it is important to note where this maximum K-S statistic is present, which the cumulative probability density above indicates to be in the low part of spectrum, i.e. close to the default score of 1. If, for example, we were to remove the low end as demonstrated below, the distributions are much closer to each other as revealed by the K-S test. However, this might also suggest that the initial \"boost\" provided with the treatment prevents more posts from immediately going stale by not being exposed to a sufficiently sizable audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1648746a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KstestResult(statistic=0.021243669687479486, pvalue=0.05855237091983819)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.kstest(control_latest[control_latest[\"score\"] > 5][\"score\"], treatment_latest[treatment_latest[\"score\"] > 5][\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3fce1",
   "metadata": {},
   "source": [
    "### Descriptive statistics\n",
    "The following section simply displays key descriptive statistics for the 2 distribution pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c963f1",
   "metadata": {},
   "source": [
    "#### Quantile statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21caaffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTROL: score\n",
      "count    20403.000000\n",
      "mean        74.736313\n",
      "std        757.735253\n",
      "min          0.000000\n",
      "25%          1.000000\n",
      "50%          2.000000\n",
      "75%         14.000000\n",
      "max      43660.000000\n",
      "Name: score, dtype: float64\n",
      "\n",
      "TREATMENT: score\n",
      "count    20461.000000\n",
      "mean        78.434681\n",
      "std        917.261883\n",
      "min         -1.000000\n",
      "25%          1.000000\n",
      "50%          3.000000\n",
      "75%         16.000000\n",
      "max      52543.000000\n",
      "Name: score, dtype: float64\n",
      "\n",
      "CONTROL: num_comments\n",
      "count    20403.000000\n",
      "mean        12.702544\n",
      "std        252.044476\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          2.000000\n",
      "75%          8.000000\n",
      "max      32770.000000\n",
      "Name: num_comments, dtype: float64\n",
      "\n",
      "TREATMENT: num_comments\n",
      "count    20461.000000\n",
      "mean        11.601193\n",
      "std         72.290503\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          2.000000\n",
      "75%          8.000000\n",
      "max       6577.000000\n",
      "Name: num_comments, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for attr in (\"score\", \"num_comments\"):\n",
    "    for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest)):\n",
    "        print(f\"{name}: {attr}\")\n",
    "        print(df[attr].describe())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bcd908",
   "metadata": {},
   "source": [
    "#### Skewness and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a0b5627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRIC: skew\n",
      "CONTROL score skew: 30.159499265889927\n",
      "TREATMENT score skew: 39.02128430525875\n",
      "CONTROL num_comments skew: 112.6626140783154\n",
      "TREATMENT num_comments skew: 53.80576525946121\n",
      "\n",
      "METRIC: kurtosis\n",
      "CONTROL score kurtosis: 1204.5628841172986\n",
      "TREATMENT score kurtosis: 1880.8687465275088\n",
      "CONTROL num_comments kurtosis: 14165.359564763241\n",
      "TREATMENT num_comments kurtosis: 4150.847243340024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for metric in (\"skew\", \"kurtosis\"):\n",
    "    print(f\"METRIC: {metric}\")\n",
    "    print(\n",
    "        \"\\n\".join(\n",
    "            (\n",
    "                f\"{name} {attr} {metric}: {getattr(df[attr], metric)()}\"\n",
    "                for attr in (\"score\", \"num_comments\")\n",
    "                for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf6c0a",
   "metadata": {},
   "source": [
    "#### (score, num_comments)-correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "447d7e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTROL:\n",
      "0.12836041716838892\n",
      "\n",
      "TREATMENT:\n",
      "0.19512985296467653\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest)):\n",
    "    print(f\"{name}:\")\n",
    "    print(df[\"score\"].corr(df[\"num_comments\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8588a",
   "metadata": {},
   "source": [
    "#### Filter extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19eb0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top(df: pd.DataFrame, attr: str, qt: float = 0.95):\n",
    "    return df[df[attr] <= df[attr].quantile(qt)][attr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b4d0f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTROL: score\n",
      "count    19387.000000\n",
      "mean        13.498891\n",
      "std         28.500464\n",
      "min          0.000000\n",
      "25%          1.000000\n",
      "50%          2.000000\n",
      "75%         10.000000\n",
      "max        185.000000\n",
      "Name: score, dtype: float64\n",
      "\n",
      "TREATMENT: score\n",
      "count    19438.000000\n",
      "mean        14.505093\n",
      "std         30.318271\n",
      "min         -1.000000\n",
      "25%          1.000000\n",
      "50%          2.000000\n",
      "75%         12.000000\n",
      "max        200.000000\n",
      "Name: score, dtype: float64\n",
      "\n",
      "CONTROL: num_comments\n",
      "count    19399.000000\n",
      "mean         4.629517\n",
      "std          6.786161\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          2.000000\n",
      "75%          6.000000\n",
      "max         36.000000\n",
      "Name: num_comments, dtype: float64\n",
      "\n",
      "TREATMENT: num_comments\n",
      "count    19438.000000\n",
      "mean         5.216895\n",
      "std          7.557649\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          2.000000\n",
      "75%          7.000000\n",
      "max         40.000000\n",
      "Name: num_comments, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for attr in (\"score\", \"num_comments\"):\n",
    "    for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest)):\n",
    "        print(f\"{name}: {attr}\")\n",
    "        print(df[df[attr] <= df[attr].quantile(0.95)][attr].describe())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22f4b814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRIC: skew\n",
      "CONTROL score skew: 3.361921626768853\n",
      "TREATMENT score skew: 3.3641578472507367\n",
      "CONTROL num_comments skew: 2.139236024249615\n",
      "TREATMENT num_comments skew: 2.135365055932428\n",
      "\n",
      "METRIC: kurtosis\n",
      "CONTROL score kurtosis: 12.186167311830408\n",
      "TREATMENT score kurtosis: 12.378922519520405\n",
      "CONTROL num_comments kurtosis: 4.681956916648158\n",
      "TREATMENT num_comments kurtosis: 4.659125814930838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for metric in (\"skew\", \"kurtosis\"):\n",
    "    print(f\"METRIC: {metric}\")\n",
    "    print(\n",
    "        \"\\n\".join(\n",
    "            (\n",
    "                f\"{name} {attr} {metric}: {getattr(df[df[attr] <= df[attr].quantile(0.95)][attr], metric)()}\"\n",
    "                for attr in (\"score\", \"num_comments\")\n",
    "                for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d31a1e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTROL:\n",
      "0.3000560401782754\n",
      "\n",
      "TREATMENT:\n",
      "0.2806298949943982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest)):\n",
    "    print(f\"{name}:\")\n",
    "    print(filter_top(df, \"score\").corr(filter_top(df, \"num_comments\")))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa436805",
   "metadata": {},
   "source": [
    "## Temporal development <a class=\"anchor\" id=\"temporal-development\"></a>\n",
    "Finally, temporal aspects of the treatment are quickly considered. In other words, does the treatment affect how long it takes for a submission to reach the state at which the majority of its score has been achieved, and is thus saturated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2b0ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "px.scatter(joined, x=\"age\", y=\"score_saturation\", color=\"group\", opacity=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "514ff875",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_sat = control[control[\"score_saturation\"] != float(\"-inf\")][(~control[\"score_saturation\"].isna())][\"score_saturation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7218cad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\AppData\\Local\\Temp\\ipykernel_10312\\540118030.py:1: UserWarning:\n",
      "\n",
      "Boolean Series key will be reindexed to match DataFrame index.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "treatment_sat = treatment[treatment[\"score_saturation\"] != float(\"-inf\")][(~treatment[\"score_saturation\"].isna())][\"score_saturation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f8b22",
   "metadata": {},
   "source": [
    "However, by looking at the summary statistics below of the two saturation distributions, they are revealed to be somewhat similar, where there seems to be some small indication that it takes ever so slightly longer for treated submissions to reach saturation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c7de71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-S TEST: KstestResult(statistic=0.08479702552613677, pvalue=0.0) \n",
      "\n",
      "CONTROL\n",
      "count    1.442876e+06\n",
      "mean     8.205579e-01\n",
      "std      3.080777e-01\n",
      "min      0.000000e+00\n",
      "25%      8.000000e-01\n",
      "50%      9.874608e-01\n",
      "75%      1.000000e+00\n",
      "max      1.000000e+00\n",
      "Name: score_saturation, dtype: float64\n",
      "\n",
      "TREATMENT\n",
      "count    1.298280e+06\n",
      "mean     8.081895e-01\n",
      "std      3.268919e-01\n",
      "min     -1.000000e+00\n",
      "25%      7.638889e-01\n",
      "50%      9.523810e-01\n",
      "75%      1.000000e+00\n",
      "max      1.000000e+00\n",
      "Name: score_saturation, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"K-S TEST:\", stats.kstest(control_sat, treatment_sat), \"\\n\")\n",
    "for name, sat in ((\"CONTROL\", control_sat), (\"TREATMENT\", treatment_sat)):\n",
    "    print(f\"{name}\")\n",
    "    print(sat.describe())\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
