{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb9df05",
   "metadata": {},
   "source": [
    "# Social Influence Project - Reddit Submission Popularity RCT <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "\n",
    "This notebook concretely illustrates the data gathering and analysis for which the main paper is based upon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fbe8a2",
   "metadata": {},
   "source": [
    "# Table of contents:\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759aabd",
   "metadata": {},
   "source": [
    "# Data gathering setup and pipeline\n",
    "\n",
    "The following section contains all the relevant python code used to interface with reddit and store the resulting data in a local SQLite database. Keep in mind that the code is not meant to be deployed and run from within a jupyter notebook. Instead, it was designed such that the entrypoint `main.py` can be run on a schedule with a cronjob. For a greater viewing experience and better overview, it is recommended to visit [the GitHub repo](https://github.com/NValsted/RDS-Project-2022-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf96405f",
   "metadata": {},
   "source": [
    "### Database interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48cdb0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#src/database.py\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, TypeVar, List, Type\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.sql.schema import Table\n",
    "from sqlmodel import create_engine, SQLModel, Session\n",
    "\n",
    "ModelType = TypeVar(\"ModelType\", bound=SQLModel)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Database:\n",
    "    \"\"\"\n",
    "    Database class with methods to create/drop tables and add/retrieve table entries\n",
    "    \"\"\"\n",
    "    engine: Engine\n",
    "\n",
    "    @contextmanager\n",
    "    def session(self):\n",
    "        with Session(self.engine) as session:\n",
    "            yield session\n",
    "\n",
    "    def create_tables(self, tables: Optional[List[Table]] = None) -> None:\n",
    "        SQLModel.metadata.create_all(self.engine, tables=tables)\n",
    "\n",
    "    def drop_tables(self, tables: Optional[List[Table]] = None) -> None:\n",
    "        SQLModel.metadata.drop_all(self.engine, tables=tables)\n",
    "\n",
    "    def add(self, instances: List[ModelType]) -> None:\n",
    "        with self.session() as session:\n",
    "            session.add_all(instances)\n",
    "            session.commit()\n",
    "\n",
    "    def get(self, model: Type[ModelType], id: int) -> Optional[ModelType]:\n",
    "        with Session(self.engine) as session:\n",
    "            matches = session.query(model).filter(model.id == id).all()\n",
    "            if len(matches) > 1:\n",
    "                raise ValueError(\n",
    "                    f\"Multiple matches for {id=} in {model.__name__}:\\n{matches}\"\n",
    "                )\n",
    "            elif len(matches) == 0:\n",
    "                return None\n",
    "            return matches[0]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DBFactory:\n",
    "    \"\"\"\n",
    "    Factory to create Database instances\n",
    "    \"\"\"\n",
    "    engine_url: str = \"sqlite:///../database.db\"\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Database:\n",
    "        engine = create_engine(url=self.engine_url, **kwargs)\n",
    "        return Database(engine=engine, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bdf539",
   "metadata": {},
   "source": [
    "### Model and database table definitions\n",
    "Python Pydantic models and SQLite table definitions are made simultaneously using the SQLModel ORM capabilities\n",
    "\n",
    "A `RedditPost` entry will be created once when a post is fetched the first time, which includes generic metadata about the post, while a `RedditPostLogPoint` will be created periodically, which is responsible for keeping track of the score and number of comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b630ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/database_models.py\n",
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "from sqlalchemy import Column, Enum as SAEnum\n",
    "from sqlmodel import SQLModel, Field\n",
    "\n",
    "\n",
    "class GroupEnum(str, Enum):\n",
    "    CONTROL = \"CONTROL\"\n",
    "    TREATMENT = \"TREATMENT\"\n",
    "\n",
    "\n",
    "class RedditPost(SQLModel):\n",
    "    id: str = Field(primary_key=True, index=True)\n",
    "    batch_id: str = Field(\n",
    "        index=True,\n",
    "        description=\"Unique ID of the batch in which the post was added\",\n",
    "    )\n",
    "    active: bool = Field(\n",
    "        default=True, description=\"Indicates whether the post is reachable\"\n",
    "    )\n",
    "    group: GroupEnum = Field(sa_column=Column(SAEnum(GroupEnum)))\n",
    "    subreddit: str = Field()\n",
    "    title: str = Field()\n",
    "    creation_date: datetime = Field(description=\"Date at which post was created\")\n",
    "\n",
    "\n",
    "class RedditPostTable(RedditPost, table=True):\n",
    "    __tablename__ = \"RedditPost\"\n",
    "\n",
    "\n",
    "class RedditPostLogPoint(SQLModel):\n",
    "    pk: Optional[int] = Field(primary_key=True, default=None, index=True)\n",
    "    id: str = Field(index=True)\n",
    "    score: int = Field()\n",
    "    num_comments: int = Field()\n",
    "    date: datetime = Field(description=\"Date at which stats were collected\")\n",
    "\n",
    "\n",
    "class RedditPostLogPointTable(RedditPostLogPoint, table=True):\n",
    "    __tablename__ = \"RedditPostLogPoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed8ba41",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "A few utility functions are also defined which are primarily concerned with increasing the robustness of the data gathering solution.\n",
    "\n",
    "The logger and its factory method provide a structured interface for saving logs persistently to disk when the main job runs for longer periods of time without manual intervention, and the `safe_call` will prevent the process from terminating immediately on any error - e.g. if a single post out of thousands in a batch raise a 404 error because it was deleted, then we still want to process the rest of the batch. Likewise, it makes sense to retry requests if the connection temporarily drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c2a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/utils.py\n",
    "from typing import Callable, Any, List, Dict, Optional\n",
    "from time import sleep\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "from prawcore import exceptions\n",
    "\n",
    "\n",
    "def get_logger(name: str = \"RDS-PROJECT\") -> logging.Logger:\n",
    "    logger = logging.getLogger(name)\n",
    "    fhandler = logging.FileHandler(filename=\"logs.log\", mode=\"a\")\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    fhandler.setFormatter(formatter)\n",
    "    logger.addHandler(fhandler)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def safe_call(\n",
    "    func: Callable,\n",
    "    args: Optional[List] = None,\n",
    "    kwargs: Optional[Dict] = None,\n",
    "    max_retries: int = 3,\n",
    "    sleep_time: int = 1,\n",
    "    exception: Exception = exceptions.NotFound,\n",
    "    raise_on_failure: bool = True,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Wraps a function and retries it if it raises an exception.\n",
    "    \"\"\"\n",
    "    logger = get_logger()\n",
    "\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    error = Exception(\"Unknown error\")\n",
    "\n",
    "    while max_retries > 0:\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except exception as e:\n",
    "            error = e\n",
    "            max_retries -= 1\n",
    "            logger.info(\n",
    "                f\"{func.__name__} failed with args {args} and kwargs {kwargs}\\n\"\n",
    "                f\"{e}\\n{traceback.format_exc()}\"\n",
    "                f\"{max_retries} retries left\"\n",
    "            )\n",
    "            sleep(sleep_time)\n",
    "\n",
    "    if raise_on_failure:\n",
    "        logger.error(f\"Failed to execute function {func.__name__}\")\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e0f73",
   "metadata": {},
   "source": [
    "### The reddit interface\n",
    "Interfacing with reddit is done via the praw reddit API wrapper, which in turn is wrapped in the RedditBot class. This class provides methods to:\n",
    "- Fetch new posts:\n",
    "    - A random batch of new posts can be fetched with `get_batch_of_posts`\n",
    "    - This batch of posts can be randomly divided into CONTROL and TREATMENT groups with `group_posts`\n",
    "- Interface with database:\n",
    "    - Add:\n",
    "        - Posts (i.e. metadata about subreddit, creation_date, CONTROL/TREATMENT group, etc.) can be added with `add_posts_to_db`\n",
    "        - Log points (i.e. observations of score and number of comments) can be added with `add_log_points`\n",
    "    - Get:\n",
    "        - Posts no older than a certain amount of days can be retrieved from the database with `get_stored_posts`\n",
    "- The data (title, score, etc.) for a list of posts can be fetched given a list of ids with `get_posts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a39c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/reddit_bot.py\n",
    "import os\n",
    "import random\n",
    "import traceback\n",
    "from datetime import datetime, timedelta\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from uuid import uuid4\n",
    "import json\n",
    "\n",
    "import praw\n",
    "\n",
    "# Local imports suppressed in notebook cells since the objects are already available in scope.\n",
    "# from src.database import DBFactory\n",
    "# from src.database_models import RedditPostTable, RedditPostLogPointTable, GroupEnum\n",
    "# from src.utils import get_logger, safe_call\n",
    "\n",
    "CLIENT_ID = os.getenv(\"CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"CLIENT_SECRET\")\n",
    "USERNAME = os.getenv(\"USERNAME\")\n",
    "PASSWORD = os.getenv(\"PASSWORD\")\n",
    "USER_AGENT = os.getenv(\"USER_AGENT\")\n",
    "RATELIMIT = int(os.getenv(\"RATELIMIT\", 5))\n",
    "\n",
    "logger = get_logger(\"REDDIT-BOT\")\n",
    "\n",
    "\n",
    "class RedditBot:\n",
    "    \"\"\"\n",
    "    Wrapper for the Reddit bot.\n",
    "\n",
    "    It contains the following methods:\n",
    "    - get_batch_of_posts: Selects a batch of posts for the experiment\n",
    "    - group_posts: Groups a list of posts into treatment and control groups\n",
    "    - add_posts_to_db: Adds a list of posts to the database\n",
    "    - add_log_points: Adds a list of log points to the database\n",
    "    - get_stored_posts: Fetches posts from the database given a date filter\n",
    "    - get_posts: Fetches posts from the Reddit API given a list of ids\n",
    "    \"\"\"\n",
    "\n",
    "    reddit: praw.Reddit\n",
    "    url: str = \"https://www.reddit.com\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        client_id: str = CLIENT_ID,\n",
    "        client_secret: str = CLIENT_SECRET,\n",
    "        username: str = USERNAME,\n",
    "        password: str = PASSWORD,\n",
    "        user_agent: str = USER_AGENT,\n",
    "        ratelimit: int = RATELIMIT,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Authenticates the bot and initializes the Reddit instance.\n",
    "        \"\"\"\n",
    "        assert isinstance(client_id, str)\n",
    "        assert isinstance(client_secret, str)\n",
    "        assert isinstance(username, str)\n",
    "        assert isinstance(password, str)\n",
    "        assert isinstance(user_agent, str)\n",
    "        assert isinstance(ratelimit, int)\n",
    "\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret,\n",
    "            username=username,\n",
    "            password=password,\n",
    "            user_agent=user_agent,\n",
    "            ratelimit=ratelimit,\n",
    "        )\n",
    "\n",
    "    def get_batch_of_posts(\n",
    "        self,\n",
    "        subreddit: str = \"all\",\n",
    "        score: int = 1,\n",
    "        num_comments: int = 1,\n",
    "        batch_size: int = 64,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Selects a batch of posts with at most 'score' number of upvotes and\n",
    "        'num_comments' number of comments in the given subreddit.\n",
    "\n",
    "        NOTE: batch_size is an upper bound on the number of posts returned.\n",
    "        \"\"\"\n",
    "\n",
    "        posts = [\n",
    "            post\n",
    "            for post in self.reddit.subreddit(subreddit).new(limit=batch_size)\n",
    "            if post.score <= score and post.num_comments <= num_comments\n",
    "        ]\n",
    "\n",
    "        return posts\n",
    "\n",
    "    @staticmethod\n",
    "    def group_posts(\n",
    "        posts: List[praw.models.Submission],\n",
    "    ) -> Tuple[List[praw.models.Submission], List[praw.models.Submission]]:\n",
    "        \"\"\"\n",
    "        Assigns posts into treatment and control groups.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_id = str(uuid4())\n",
    "        random.shuffle(posts)\n",
    "        if len(posts) % 2 != 0:\n",
    "            posts.pop()  # Drop a random post to make the list even\n",
    "\n",
    "        middle = len(posts) // 2\n",
    "        treatment_posts = posts[:middle]\n",
    "        control_posts = posts[middle:]\n",
    "\n",
    "        for post in treatment_posts:\n",
    "            post.upvote()\n",
    "            post.group = GroupEnum.TREATMENT\n",
    "            post.batch_id = batch_id\n",
    "\n",
    "        for post in control_posts:\n",
    "            post.group = GroupEnum.CONTROL\n",
    "            post.batch_id = batch_id\n",
    "\n",
    "        return treatment_posts, control_posts\n",
    "\n",
    "    @staticmethod\n",
    "    def add_posts_to_db(\n",
    "        posts: List[praw.models.Submission],\n",
    "        backup: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "        prepared_posts = []\n",
    "\n",
    "        def _prepare_post(post: praw.models.Submission) -> Dict:\n",
    "            return dict(\n",
    "                id=post.id,\n",
    "                batch_id=post.batch_id,\n",
    "                group=post.group,\n",
    "                subreddit=str(post.subreddit),\n",
    "                title=post.title,\n",
    "                creation_date=post.created_utc,\n",
    "            )\n",
    "\n",
    "        for post in posts:\n",
    "            prepared_post = safe_call(\n",
    "                _prepare_post,\n",
    "                args=[post],\n",
    "                exception=Exception,\n",
    "                raise_on_failure=False,\n",
    "            )\n",
    "            if prepared_post is not None:\n",
    "                prepared_posts.append(prepared_post)\n",
    "\n",
    "        if backup:\n",
    "            today = datetime.today().date().isoformat()\n",
    "            with open(f\"backup/REDDITBOT_{today}_{str(uuid4())}.json\", \"w\") as f:\n",
    "                json.dump(prepared_posts, f, indent=4, default=str)\n",
    "\n",
    "        db = DBFactory()()\n",
    "\n",
    "        parsed_posts = [RedditPostTable(**post) for post in prepared_posts]\n",
    "        db.add(parsed_posts)\n",
    "        logger.info(f\"Added {len(parsed_posts)} posts to the database\")\n",
    "\n",
    "        RedditBot.add_log_points(posts, backup=backup)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_log_points(\n",
    "        posts: List[praw.models.Submission], backup: bool = False\n",
    "    ) -> None:\n",
    "\n",
    "        prepared_posts = []\n",
    "        stale_posts = []\n",
    "\n",
    "        def _prepare_post(post: praw.models.Submission) -> Dict:\n",
    "            return dict(\n",
    "                id=post.id,\n",
    "                score=post.score,\n",
    "                num_comments=post.num_comments,\n",
    "                date=datetime.now(),\n",
    "            )\n",
    "\n",
    "        for post in posts:\n",
    "            prepared_post = safe_call(\n",
    "                _prepare_post,\n",
    "                args=[post],\n",
    "                exception=Exception,\n",
    "                raise_on_failure=False,\n",
    "            )\n",
    "            if prepared_post is not None:\n",
    "                prepared_posts.append(prepared_post)\n",
    "            else:\n",
    "                stale_posts.append(post)\n",
    "\n",
    "        if backup:\n",
    "            today = datetime.today().date().isoformat()\n",
    "            with open(f\"backup/REDDITBOT_{today}_{str(uuid4())}.json\", \"w\") as f:\n",
    "                json.dump(prepared_posts, f, indent=4, default=str)\n",
    "\n",
    "        db = DBFactory()()\n",
    "\n",
    "        parsed_posts = [RedditPostLogPointTable(**post) for post in prepared_posts]\n",
    "        db.add(parsed_posts)\n",
    "        logger.info(f\"Added {len(parsed_posts)} log points to database\")\n",
    "\n",
    "        for post in stale_posts:\n",
    "            old_instance = db.get(RedditPostTable, id=post.id)\n",
    "            if old_instance is not None:\n",
    "                old_instance.active = False\n",
    "                db.add([old_instance])\n",
    "\n",
    "        logger.info(f\"Marked {len(stale_posts)} stale posts\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_stored_posts(max_age: int = 8) -> List[RedditPostTable]:\n",
    "        db = DBFactory()()\n",
    "\n",
    "        with db.session() as session:\n",
    "            posts = (\n",
    "                session.query(RedditPostTable)\n",
    "                .filter(\n",
    "                    RedditPostTable.creation_date\n",
    "                    >= (datetime.now() - timedelta(days=max_age))\n",
    "                )\n",
    "                .filter(RedditPostTable.active)\n",
    "                .all()\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Fetched {len(posts)} active posts from the database\")\n",
    "\n",
    "            return posts\n",
    "\n",
    "    def _submission_wrapper(self, *args, **kwargs) -> Optional[praw.models.Submission]:\n",
    "        try:\n",
    "            return self.reddit.submission(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                f\"{e}\\n{traceback.format_exc()}\\nargs: {args}\\nkwargs: {kwargs}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    def get_posts(\n",
    "        self, ids: List[str], threads: int = 4\n",
    "    ) -> List[praw.models.Submission]:\n",
    "        with ThreadPool(threads) as pool:\n",
    "            posts = pool.map(self._submission_wrapper, ids)\n",
    "\n",
    "        posts = [post for post in posts if post is not None]\n",
    "\n",
    "        return posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3d40ed",
   "metadata": {},
   "source": [
    "### One-time entrypoint - `setup.py`\n",
    "The `setup` function simply establishes a connection to- and possibly creates the database, after which it drops any existing tables and creates new ones afresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32395a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup.py\n",
    "\n",
    "# Local imports suppressed in notebook cells since the objects are already available in scope.\n",
    "# from src.database_models import RedditPostTable, RedditPostLogPointTable  # NOQA : F401\n",
    "# from src.database import DBFactory\n",
    "\n",
    "\n",
    "def setup():\n",
    "    db = DBFactory()()\n",
    "    db.drop_tables()\n",
    "    db.create_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f43d3e",
   "metadata": {},
   "source": [
    "### Main entrypoint - `main.py`\n",
    "With everything set up, the `main` function defines a simple routine for monitoring the stats of previously fetched posts as well as fetching a batch of newly created posts. This is the function that is deployed to run periodically. Check [the GitHub repo](https://github.com/NValsted/RDS-Project-2022-1) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af34d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "# Local imports suppressed in notebook cells since the objects are already available in scope.\n",
    "# from src import RedditBot\n",
    "# from src.utils import safe_call\n",
    "\n",
    "\n",
    "def main():\n",
    "    bot = RedditBot()\n",
    "\n",
    "    # Update old Posts\n",
    "    posts = bot.get_stored_posts()\n",
    "    ids = {post.id for post in posts}\n",
    "\n",
    "    posts = bot.get_posts(ids)\n",
    "    bot.add_log_points(posts)\n",
    "\n",
    "    # New posts\n",
    "    treatment, control = safe_call(\n",
    "        func=lambda: bot.group_posts(bot.get_batch_of_posts())\n",
    "    )\n",
    "    bot.add_posts_to_db(treatment)\n",
    "    bot.add_posts_to_db(control)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27102d",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "At this point, over 20000 posts have been fetched and monitored over the course of 7 days each, which has resulted in around 1.5 million log points. This section marks the start of an analysis of the resulting data.\n",
    "\n",
    "A snapshot of the database is available at https://ituniversity-my.sharepoint.com/:u:/g/personal/nicv_itu_dk/ESyJlN06ZbJEsYJSpBH6zSEB8IKTH5iKjAMcHIjumsXfIQ?e=YGatIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f647ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df1c96",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d19ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_post_df = pd.read_sql_table(RedditPostTable.__tablename__, DBFactory.engine_url)\n",
    "reddit_post_log_point_df = pd.read_sql_table(RedditPostLogPointTable.__tablename__, DBFactory.engine_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94eecc",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Before commencing with the analysis, a little preprocessing is beneficial, e.g. due to the fact that certain posts are marked inactive since they have been deleted or otherwise made unreachable, which has unbalanced the dataset slightly.\n",
    "\n",
    "The preprocessing steps are the following (Which are intertwined in practice for convenience):\n",
    "- Join post info and log points (`RedditPostTable` and `RedditPostLogPointTable`)\n",
    "- Balance dataset. \n",
    "    - For each inactive post, identify a post from the conjugate group with the same `batch_id` and filter away both.\n",
    "    - Filter away any post that has not been monitored for at least 7 days (i.e. younger than 7 days or marked inactive before the 7 day mark).\n",
    "- Create derived columns: `age` and `saturation`.\n",
    "- Unbias treatment group by subtracting 1 from the score\n",
    "- For each of the two groups, create dataframes containing only the latest log point for a post.\n",
    "\n",
    "These steps will be described further in the relevant cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f326aeda",
   "metadata": {},
   "source": [
    "### Join dataframes\n",
    "A join between the RedditPost and RedditPostLogPoint on the `id` column is done (reddit's id for a given post), which essentially yields RedditPostLogPoint but with all the relevant metadata attached for the post which the log point corresponds to. A random sample of the dataframe is shown as an example of the resulting format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5d6fea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = pd.merge(reddit_post_df, reddit_post_log_point_df, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "96e67e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>id</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>active</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>pk</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>409005</th>\n",
       "      <td>TREATMENT</td>\n",
       "      <td>ta0bzb</td>\n",
       "      <td>151f376f-c0c4-46e1-9e70-e61037d1ce8d</td>\n",
       "      <td>True</td>\n",
       "      <td>DealAndSale</td>\n",
       "      <td>üî•50% Off Code ‚Äì $19.99 Waterproof Solar Panel ...</td>\n",
       "      <td>2022-03-09 05:12:40</td>\n",
       "      <td>331365</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-12 09:14:58.107552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134850</th>\n",
       "      <td>TREATMENT</td>\n",
       "      <td>t80w05</td>\n",
       "      <td>21f1f24d-4c5e-4e59-a376-d57474fd5c0a</td>\n",
       "      <td>True</td>\n",
       "      <td>danganronpa</td>\n",
       "      <td>(Future Arc) I figured out what the ‚ÄúNG‚Äù in ‚ÄúN...</td>\n",
       "      <td>2022-03-06 15:19:35</td>\n",
       "      <td>106452</td>\n",
       "      <td>515</td>\n",
       "      <td>25</td>\n",
       "      <td>2022-03-08 03:49:24.295059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998993</th>\n",
       "      <td>TREATMENT</td>\n",
       "      <td>tp5g27</td>\n",
       "      <td>08b3d48f-1c14-4501-a4a9-c133aaf20411</td>\n",
       "      <td>True</td>\n",
       "      <td>relationship_advice</td>\n",
       "      <td>I (23 m) my gf (20 f), she broke up with me a ...</td>\n",
       "      <td>2022-03-26 22:06:31</td>\n",
       "      <td>995099</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-03-30 16:05:02.038658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370403</th>\n",
       "      <td>TREATMENT</td>\n",
       "      <td>t9giz1</td>\n",
       "      <td>d684ad72-d692-4f5e-90d9-c0178e92a1c2</td>\n",
       "      <td>True</td>\n",
       "      <td>BayleyBooty</td>\n",
       "      <td>its fucking huge</td>\n",
       "      <td>2022-03-08 13:13:14</td>\n",
       "      <td>146927</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-08 15:29:02.891053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175679</th>\n",
       "      <td>TREATMENT</td>\n",
       "      <td>ttn5nk</td>\n",
       "      <td>8bad43ba-4009-4a20-8f7a-13e8bdb457de</td>\n",
       "      <td>True</td>\n",
       "      <td>DBZDokkanBattle</td>\n",
       "      <td>Global is dead</td>\n",
       "      <td>2022-04-01 10:06:33</td>\n",
       "      <td>1105329</td>\n",
       "      <td>199</td>\n",
       "      <td>53</td>\n",
       "      <td>2022-04-03 06:17:46.574560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             group      id                              batch_id  active  \\\n",
       "409005   TREATMENT  ta0bzb  151f376f-c0c4-46e1-9e70-e61037d1ce8d    True   \n",
       "134850   TREATMENT  t80w05  21f1f24d-4c5e-4e59-a376-d57474fd5c0a    True   \n",
       "998993   TREATMENT  tp5g27  08b3d48f-1c14-4501-a4a9-c133aaf20411    True   \n",
       "370403   TREATMENT  t9giz1  d684ad72-d692-4f5e-90d9-c0178e92a1c2    True   \n",
       "1175679  TREATMENT  ttn5nk  8bad43ba-4009-4a20-8f7a-13e8bdb457de    True   \n",
       "\n",
       "                   subreddit  \\\n",
       "409005           DealAndSale   \n",
       "134850           danganronpa   \n",
       "998993   relationship_advice   \n",
       "370403           BayleyBooty   \n",
       "1175679      DBZDokkanBattle   \n",
       "\n",
       "                                                     title  \\\n",
       "409005   üî•50% Off Code ‚Äì $19.99 Waterproof Solar Panel ...   \n",
       "134850   (Future Arc) I figured out what the ‚ÄúNG‚Äù in ‚ÄúN...   \n",
       "998993   I (23 m) my gf (20 f), she broke up with me a ...   \n",
       "370403                                    its fucking huge   \n",
       "1175679                                     Global is dead   \n",
       "\n",
       "              creation_date       pk  score  num_comments  \\\n",
       "409005  2022-03-09 05:12:40   331365      2             0   \n",
       "134850  2022-03-06 15:19:35   106452    515            25   \n",
       "998993  2022-03-26 22:06:31   995099      2             3   \n",
       "370403  2022-03-08 13:13:14   146927     11             0   \n",
       "1175679 2022-04-01 10:06:33  1105329    199            53   \n",
       "\n",
       "                              date  \n",
       "409005  2022-03-12 09:14:58.107552  \n",
       "134850  2022-03-08 03:49:24.295059  \n",
       "998993  2022-03-30 16:05:02.038658  \n",
       "370403  2022-03-08 15:29:02.891053  \n",
       "1175679 2022-04-03 06:17:46.574560  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b9e7db",
   "metadata": {},
   "source": [
    "### Derived columns p0 - age\n",
    "The `age` column simply denotes how long it has been since a post was created when a log point was recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d7bb0106",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined[\"age\"] = joined[\"date\"] - joined[\"creation_date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55acf5",
   "metadata": {},
   "source": [
    "### Unbias TREATMENT group\n",
    "The initial upvote of 1 is subtracted from the TREATMENT group, since we are interested in investigating whether the treatment has increased popularity as expressed by external users - i.e. all users that are not our bot.\n",
    "\n",
    "Perhaps, this is best explained with an example in the extremes: If we assume an artificial world where no other users interact with posts on the platform, then we can safely say beforehand that the treatment will not have an effect. If we then perform our experiment, all posts in the CONTROL group will have a score of 0, and all posts in the TREATMENT group will have a score of 1. If we were to perform any analysis on the resulting data without unbiasing the TREATMENT group, we would wrongfully conclude effectiveness of the TREATMENT due to the difference in score distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bdc68433",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.loc[joined[\"group\"] == GroupEnum.TREATMENT.value, (\"score\")] -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d0a1b4",
   "metadata": {},
   "source": [
    "### Latest posts\n",
    "We have record many log points for each individual post, so the `joined_latest` dataframe consists of only the latest log point for each post  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "484b3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_latest = joined.sort_values([\"date\"], ascending=False).groupby(by=\"id\", as_index=False).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f36d7d",
   "metadata": {},
   "source": [
    "### Balance dataset\n",
    "To balance the dataset, we want remove any posts that have not yet been tracked for at least 7 days, and we also want to ensure that there are equally many data points in the control and treatment groups. Since we sample an equal amount of control and treatment posts in each batch, the only reason these can be different is if one or more posts have been marked inactive (i.e. deleted or otherwise unreachable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "034c6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_drop = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa300b",
   "metadata": {},
   "source": [
    "First, filter posts younger than 7 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6443046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in joined_latest[joined_latest[\"age\"] < timedelta(days=7)].iterrows():\n",
    "    ids_to_drop.add(str(row[\"id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6644a7f",
   "metadata": {},
   "source": [
    "Then balance pairs within batches, prioritizing removing conjugate posts which are also inactive - e.g. if a batch contains a single inactive control post and inactive treatment post, then these two should simply cancel out. Otherwise, simply sample a random active post of the conjugate group to cancel out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cf365747",
   "metadata": {},
   "outputs": [],
   "source": [
    "inactive_posts = joined_latest[joined_latest[\"active\"] == False]\n",
    "\n",
    "for group in inactive_posts.groupby(by=\"batch_id\", as_index=False):\n",
    "    group_key, group_df = group\n",
    "\n",
    "    control_remainder, treatment_remainder = (\n",
    "        joined_latest[\n",
    "            (joined_latest[\"batch_id\"] == group_key)\n",
    "            & (joined_latest[\"group\"] == group.value)\n",
    "            & ~(joined_latest[\"id\"].isin(set(group_df[\"id\"])))\n",
    "        ]\n",
    "        for group in (GroupEnum.CONTROL, GroupEnum.TREATMENT)\n",
    "    )\n",
    "    num_control = control_remainder.shape[0]\n",
    "    num_treatment = treatment_remainder.shape[0]\n",
    "    \n",
    "    if num_control != num_treatment:\n",
    "        conjugate_remainder = control_remainder if num_treatment < num_control else treatment_remainder\n",
    "        num_to_drop = abs(num_control - num_treatment)\n",
    "        to_drop = conjugate_remainder.sample(n=num_to_drop)\n",
    "        ids_to_drop.add(str(to_drop[\"id\"]))\n",
    "        \n",
    "    for entry in group_df[\"id\"]:\n",
    "        ids_to_drop.add(str(entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1663f278",
   "metadata": {},
   "source": [
    "Apply filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "869a228f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 4046 post(s)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dropping {len(ids_to_drop)} post(s)\")\n",
    "\n",
    "joined_latest = joined_latest[~(joined_latest[\"id\"].isin(ids_to_drop))]\n",
    "# joined = joined[~(joined[\"id\"].isin(ids_to_drop))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1ecf4",
   "metadata": {},
   "source": [
    "### Derived columns p1 - saturation\n",
    "Saturation is useful in a temporal context and describes the ratio of some maximum value for a post. E.g. a post with `score` values 0, 50, and 100 will be mapped to `score_saturation` values of 0, 0.5, 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9ac30f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined[\"score_saturation\"] = joined[\"score\"] / joined.groupby(\"id\")[\"score\"].transform(np.max)\n",
    "joined[\"num_comments_saturation\"] = joined[\"num_comments\"] / joined.groupby(\"id\")[\"num_comments\"].transform(np.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ede8aa",
   "metadata": {},
   "source": [
    "### Overview of resulting dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4771b2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>id</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>active</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>pk</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>date</th>\n",
       "      <th>age</th>\n",
       "      <th>score_saturation</th>\n",
       "      <th>num_comments_saturation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>767375</th>\n",
       "      <td>TREATMENT</td>\n",
       "      <td>ti2bl1</td>\n",
       "      <td>bda7a90c-4b82-4cfe-8848-14a8f5fc5c75</td>\n",
       "      <td>True</td>\n",
       "      <td>natureporn</td>\n",
       "      <td>Bijela, Bosnia and Herzegovina</td>\n",
       "      <td>2022-03-19 19:06:44</td>\n",
       "      <td>733526</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-22 15:48:54.255870</td>\n",
       "      <td>2 days 20:42:10.255870</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944012</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>tndnmc</td>\n",
       "      <td>db09c191-5065-4a2e-8203-7eaf8e3ad80a</td>\n",
       "      <td>True</td>\n",
       "      <td>crossfit</td>\n",
       "      <td>Am I the only idiot that can‚Äôt figure out how ...</td>\n",
       "      <td>2022-03-25 04:06:48</td>\n",
       "      <td>828782</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-03-25 15:08:55.277513</td>\n",
       "      <td>0 days 11:02:07.277513</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388521</th>\n",
       "      <td>TREATMENT</td>\n",
       "      <td>t9mqzs</td>\n",
       "      <td>6cf848ad-85d1-4660-9352-3eced31bcbe7</td>\n",
       "      <td>True</td>\n",
       "      <td>autotldr</td>\n",
       "      <td>Biden set to ban Russian oil under pressure fr...</td>\n",
       "      <td>2022-03-08 18:00:40</td>\n",
       "      <td>350172</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-12 18:12:55.308119</td>\n",
       "      <td>4 days 00:12:15.308119</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38470</th>\n",
       "      <td>TREATMENT</td>\n",
       "      <td>t7due5</td>\n",
       "      <td>4ffbf91c-01c8-444d-b1cf-f70095977a11</td>\n",
       "      <td>True</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>You are a main author in for 2000's cartoon. W...</td>\n",
       "      <td>2022-03-05 17:04:58</td>\n",
       "      <td>38575</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-03-07 02:21:32.407340</td>\n",
       "      <td>1 days 09:16:34.407340</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071130</th>\n",
       "      <td>TREATMENT</td>\n",
       "      <td>tqrbze</td>\n",
       "      <td>be3e6add-00f3-4548-90f5-33143fae2199</td>\n",
       "      <td>True</td>\n",
       "      <td>Wonderlands</td>\n",
       "      <td>Seeing that Top 1 always makes me feel good.</td>\n",
       "      <td>2022-03-29 04:06:54</td>\n",
       "      <td>1194339</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2022-04-06 03:08:39.028365</td>\n",
       "      <td>7 days 23:01:45.028365</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             group      id                              batch_id  active  \\\n",
       "767375   TREATMENT  ti2bl1  bda7a90c-4b82-4cfe-8848-14a8f5fc5c75    True   \n",
       "944012     CONTROL  tndnmc  db09c191-5065-4a2e-8203-7eaf8e3ad80a    True   \n",
       "388521   TREATMENT  t9mqzs  6cf848ad-85d1-4660-9352-3eced31bcbe7    True   \n",
       "38470    TREATMENT  t7due5  4ffbf91c-01c8-444d-b1cf-f70095977a11    True   \n",
       "1071130  TREATMENT  tqrbze  be3e6add-00f3-4548-90f5-33143fae2199    True   \n",
       "\n",
       "           subreddit                                              title  \\\n",
       "767375    natureporn                     Bijela, Bosnia and Herzegovina   \n",
       "944012      crossfit  Am I the only idiot that can‚Äôt figure out how ...   \n",
       "388521      autotldr  Biden set to ban Russian oil under pressure fr...   \n",
       "38470      AskReddit  You are a main author in for 2000's cartoon. W...   \n",
       "1071130  Wonderlands       Seeing that Top 1 always makes me feel good.   \n",
       "\n",
       "              creation_date       pk  score  num_comments  \\\n",
       "767375  2022-03-19 19:06:44   733526     22             0   \n",
       "944012  2022-03-25 04:06:48   828782      2             2   \n",
       "388521  2022-03-08 18:00:40   350172      1             0   \n",
       "38470   2022-03-05 17:04:58    38575      0             4   \n",
       "1071130 2022-03-29 04:06:54  1194339      0             6   \n",
       "\n",
       "                              date                    age  score_saturation  \\\n",
       "767375  2022-03-22 15:48:54.255870 2 days 20:42:10.255870              0.88   \n",
       "944012  2022-03-25 15:08:55.277513 0 days 11:02:07.277513              1.00   \n",
       "388521  2022-03-12 18:12:55.308119 4 days 00:12:15.308119              1.00   \n",
       "38470   2022-03-07 02:21:32.407340 1 days 09:16:34.407340              0.00   \n",
       "1071130 2022-04-06 03:08:39.028365 7 days 23:01:45.028365              0.00   \n",
       "\n",
       "         num_comments_saturation  \n",
       "767375                       NaN  \n",
       "944012                       1.0  \n",
       "388521                       NaN  \n",
       "38470                        1.0  \n",
       "1071130                      1.0  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38ac38",
   "metadata": {},
   "source": [
    "### Split dataframe in groups\n",
    "Finally, we simply create dataframes containing only values from the respective groups for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4e906a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "control, treatment = (joined[joined[\"group\"] == group.value] for group in (GroupEnum.CONTROL, GroupEnum.TREATMENT))\n",
    "control_latest, treatment_latest = (\n",
    "    joined_latest[joined_latest[\"group\"] == group.value] for group in (GroupEnum.CONTROL, GroupEnum.TREATMENT)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0545e336",
   "metadata": {},
   "source": [
    "## Distribution similarity\n",
    "The following section investigates the similarity between the score and number of comments distributions between the two groups, in order to evaluate the effectiveness of the treatment.\n",
    "\n",
    "The following plot displays histograms of the data points within each group. The general distribution type seems similar, but parameters seem to be noticeably different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "86a2c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import io as pio, graph_objects as go, express as px\n",
    "from plotly.subplots import make_subplots\n",
    "pio.renderers.default = \"iframe\"\n",
    "# Jupyter notebooks don't handle plotly figures well.\n",
    "# Therefore, iframes and %%capture are used to save the resulting html files to disk instead.\n",
    "# These should appear in the iframe_figures directory, following the figure_{cell_number}.html naming scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "56afc4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    shared_yaxes=True,\n",
    "    subplot_titles=(\"Score distribution\", \"Number of comments distribution\"),\n",
    "    horizontal_spacing=0.05,\n",
    ")\n",
    "\n",
    "for i in (1, 2):\n",
    "    for j, attr in ((1, \"score\"), (2, \"num_comments\")):\n",
    "        for df, color in ((control_latest, \"#4969AA\"), (treatment_latest, \"#C90D0D\")):\n",
    "            fig.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=df[attr],\n",
    "                    histnorm=\"percent\",\n",
    "                    name=df[\"group\"].min(),\n",
    "                    xbins=dict(\n",
    "                        start=df[attr].min(),\n",
    "                        end=df[attr].max(),\n",
    "                        size=0.5\n",
    "                    ),\n",
    "                    marker_color=color,\n",
    "                    opacity=0.75,\n",
    "                    showlegend=i == 1 and j == 1\n",
    "                ),\n",
    "                row=i,\n",
    "                col=j,\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            yaxis_title_text=\"Percent\",\n",
    "            bargap=0.2,\n",
    "            bargroupgap=0.1\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(type=\"log\", row=1, col=1)\n",
    "fig.update_xaxes(type=\"log\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Score (log)\", row=2, col=1, type=\"log\")\n",
    "fig.update_xaxes(title_text=\"Number of comments (log)\", row=2, col=2, type=\"log\")\n",
    "\n",
    "fig.update_yaxes(title_text=\"Percent (log)\", type=\"log\", row=2, col=1)\n",
    "fig.update_yaxes(type=\"log\", row=2, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b615d28c",
   "metadata": {},
   "source": [
    "### Kolmogorov-Smirnov test\n",
    "With the Kolmogorov-Smirnov test, we perform an empirical test to investigate whether the data points appear to belong to the same distribution. The null hypothesis is that the distributions are identical, and thus a low p-value indicates that there is a significant difference between the control and treatment groups. As can be seen below, the results are in favour of the alternative hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "00dafe30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KstestResult(statistic=0.05385415462845256, pvalue=2.6247763454387786e-11)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.kstest(control_latest[\"score\"], treatment_latest[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "261bf17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KstestResult(statistic=0.03548486826193091, pvalue=3.7392165653535435e-05)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.kstest(control_latest[\"num_comments\"], treatment_latest[\"num_comments\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc95ae8",
   "metadata": {},
   "source": [
    "However, small fluctuations might provide an inaccurate picture - e.g. a post with a score of 34 is probably not significantly different from one with a score of 35. So we will investigate whether rounding the score and number of comments to the nearest multiple of a range of numbers will jeopardize the result above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0cc1c7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:47<00:00,  5.29s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:43<00:00,  4.87s/it]\n"
     ]
    }
   ],
   "source": [
    "binned_ks_test_score = [stats.kstest(*(df[\"score\"].map(lambda x: i * round(x / i)) for df in (control_latest, treatment_latest))).pvalue for i in tqdm(range(1, 10))]\n",
    "binned_ks_test_num_comments = [stats.kstest(*(df[\"num_comments\"].map(lambda x: i * round(x / i)) for df in (control_latest, treatment_latest))).pvalue for i in tqdm(range(1, 10))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c839c",
   "metadata": {},
   "source": [
    "...But as the plot below shows, even pessimistically trying several different bin sizes, the maximum p-value is still no larger than on the order of $10^{-3}$, which occurs for the score distributions when rounding the score to the nearest multiple of 5. Thus, it seems more likely that there is a significant difference between the control and treatment groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2dcc671c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_140.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    subplot_titles=(\"KS-test binned score distributions\", \"KS-test binned number of comments distributions\"),\n",
    ")\n",
    "for i, lst in enumerate((binned_ks_test_score, binned_ks_test_num_comments)):\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, len(lst) + 1)), y=lst), row=i+1, col=1)\n",
    "fig.update_yaxes(type=\"log\", row=\"all\", col=\"all\")\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3fce1",
   "metadata": {},
   "source": [
    "### Descriptive statistics\n",
    "The following section simply displays key descriptive statistics for the 2 distribution pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c963f1",
   "metadata": {},
   "source": [
    "#### Quantile statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "21caaffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTROL: score\n",
      "count     8583.000000\n",
      "mean        66.090994\n",
      "std        577.805053\n",
      "min          0.000000\n",
      "25%          1.000000\n",
      "50%          2.000000\n",
      "75%         14.000000\n",
      "max      28010.000000\n",
      "Name: score, dtype: float64\n",
      "\n",
      "TREATMENT: score\n",
      "count     8653.000000\n",
      "mean        71.197041\n",
      "std        705.669589\n",
      "min         -1.000000\n",
      "25%          1.000000\n",
      "50%          3.000000\n",
      "75%         16.000000\n",
      "max      46709.000000\n",
      "Name: score, dtype: float64\n",
      "\n",
      "CONTROL: num_comments\n",
      "count     8583.000000\n",
      "mean        13.459047\n",
      "std        356.097275\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          2.000000\n",
      "75%          7.000000\n",
      "max      32770.000000\n",
      "Name: num_comments, dtype: float64\n",
      "\n",
      "TREATMENT: num_comments\n",
      "count    8653.000000\n",
      "mean       11.984861\n",
      "std        95.288079\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         2.000000\n",
      "75%         8.000000\n",
      "max      6577.000000\n",
      "Name: num_comments, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for attr in (\"score\", \"num_comments\"):\n",
    "    for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest)):\n",
    "        print(f\"{name}: {attr}\")\n",
    "        print(df[attr].describe())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bcd908",
   "metadata": {},
   "source": [
    "#### Skewness and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5a0b5627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRIC: skew\n",
      "CONTROL score skew: 25.830230227857694\n",
      "TREATMENT score skew: 41.480538097552596\n",
      "CONTROL num_comments skew: 90.75046398862972\n",
      "TREATMENT num_comments skew: 50.795881485103315\n",
      "\n",
      "METRIC: kurtosis\n",
      "CONTROL score kurtosis: 906.1568988458805\n",
      "TREATMENT score kurtosis: 2381.0497768056844\n",
      "CONTROL num_comments kurtosis: 8346.061735457748\n",
      "TREATMENT num_comments kurtosis: 3139.3810454998165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for metric in (\"skew\", \"kurtosis\"):\n",
    "    print(f\"METRIC: {metric}\")\n",
    "    print(\n",
    "        \"\\n\".join(\n",
    "            (\n",
    "                f\"{name} {attr} {metric}: {getattr(df[attr], metric)()}\"\n",
    "                for attr in (\"score\", \"num_comments\")\n",
    "                for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf6c0a",
   "metadata": {},
   "source": [
    "#### (score, num_comments)-correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "447d7e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTROL:\n",
      "0.07851519321759166\n",
      "\n",
      "TREATMENT:\n",
      "0.14937576884249684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest)):\n",
    "    print(f\"{name}:\")\n",
    "    print(df[\"score\"].corr(df[\"num_comments\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8588a",
   "metadata": {},
   "source": [
    "#### Filter extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "19eb0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top(df: pd.DataFrame, attr: str, qt: float = 0.95):\n",
    "    return df[df[attr] <= df[attr].quantile(qt)][attr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2b4d0f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTROL: score\n",
      "count    8153.000000\n",
      "mean       13.143137\n",
      "std        27.702794\n",
      "min         0.000000\n",
      "25%         1.000000\n",
      "50%         2.000000\n",
      "75%        10.000000\n",
      "max       184.000000\n",
      "Name: score, dtype: float64\n",
      "\n",
      "TREATMENT: score\n",
      "count    8220.000000\n",
      "mean       14.498905\n",
      "std        30.120945\n",
      "min        -1.000000\n",
      "25%         1.000000\n",
      "50%         2.000000\n",
      "75%        12.000000\n",
      "max       198.000000\n",
      "Name: score, dtype: float64\n",
      "\n",
      "CONTROL: num_comments\n",
      "count    8155.000000\n",
      "mean        4.484365\n",
      "std         6.556211\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         2.000000\n",
      "75%         6.000000\n",
      "max        34.000000\n",
      "Name: num_comments, dtype: float64\n",
      "\n",
      "TREATMENT: num_comments\n",
      "count    8223.000000\n",
      "mean        5.177429\n",
      "std         7.452877\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         2.000000\n",
      "75%         7.000000\n",
      "max        40.000000\n",
      "Name: num_comments, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for attr in (\"score\", \"num_comments\"):\n",
    "    for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest)):\n",
    "        print(f\"{name}: {attr}\")\n",
    "        print(df[df[attr] <= df[attr].quantile(0.95)][attr].describe())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "22f4b814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRIC: skew\n",
      "CONTROL score skew: 3.438887605605816\n",
      "TREATMENT score skew: 3.3554357393754417\n",
      "CONTROL num_comments skew: 2.1308976390012226\n",
      "TREATMENT num_comments skew: 2.144510730065316\n",
      "\n",
      "METRIC: kurtosis\n",
      "CONTROL score kurtosis: 12.9692022378779\n",
      "TREATMENT score kurtosis: 12.304818847783517\n",
      "CONTROL num_comments kurtosis: 4.579087890105466\n",
      "TREATMENT num_comments kurtosis: 4.763500495194618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for metric in (\"skew\", \"kurtosis\"):\n",
    "    print(f\"METRIC: {metric}\")\n",
    "    print(\n",
    "        \"\\n\".join(\n",
    "            (\n",
    "                f\"{name} {attr} {metric}: {getattr(df[df[attr] <= df[attr].quantile(0.95)][attr], metric)()}\"\n",
    "                for attr in (\"score\", \"num_comments\")\n",
    "                for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d31a1e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTROL:\n",
      "0.2811512893667608\n",
      "\n",
      "TREATMENT:\n",
      "0.29734789310834914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest)):\n",
    "    print(f\"{name}:\")\n",
    "    print(filter_top(df, \"score\").corr(filter_top(df, \"num_comments\")))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa436805",
   "metadata": {},
   "source": [
    "## Temporal similarity\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b2b0ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "px.scatter(joined, x=\"age\", y=\"score_saturation\", color=\"group\", opacity=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "514ff875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8121326617171523"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control[\"score_saturation\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7218cad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment[\"score_saturation\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b47481a",
   "metadata": {},
   "source": [
    "# Draft conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d96ffa",
   "metadata": {},
   "source": [
    "In conclusion: From the KS-tests we deduce that the probability of the samples from the two distribution pairs - (TREATMENT_score, CONTROL_score) and (TREATMENT_num_comments, CONTROL_num_comments) - originating from the respective same distribution are both significantly less than 5% (p-value < 0.05). Investigating descriptive statistics of these distributions leads us to believe that the popularity of a post in terms of its score is indeed higher for posts in the treatment group. However, the opposite is true for the number of comments. Initially, we had hypothesized that score and number of comments were correlated, which does not seem to be the case, and this does tie together with the aforementioned uncorrelated effectiveness of the treatment on score and comments. However, a word of caution: The distributions are severely heavy-tailed as indicated by the kurtosis measures, and thus the extreme sample values can easily impact the results significantly, given the fairly limited size of the sample. This is demonstrated by filtering away the top 5% of the data from each distribution, yielding noticeably different perceived relationships between the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab1921f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
