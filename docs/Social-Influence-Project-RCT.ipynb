{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb9df05",
   "metadata": {},
   "source": [
    "# Social Influence Project - Reddit Submission Popularity RCT <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "\n",
    "This notebook concretely illustrates the data gathering and analysis for which the main paper is based upon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fbe8a2",
   "metadata": {},
   "source": [
    "# Table of contents:\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759aabd",
   "metadata": {},
   "source": [
    "# Data gathering setup and pipeline\n",
    "\n",
    "The following section contains all the relevant python code used to interface with reddit and store the resulting data in a local SQLite database. Keep in mind that the code is not meant to be deployed and run from within a jupyter notebook. Instead, it was designed such that the entrypoint `main.py` can be run on a schedule with a cronjob. For a greater viewing experience and better overview, it is recommended to visit [the GitHub repo](https://github.com/NValsted/RDS-Project-2022-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf96405f",
   "metadata": {},
   "source": [
    "### Database interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48cdb0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#src/database.py\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, TypeVar, List, Type\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.sql.schema import Table\n",
    "from sqlmodel import create_engine, SQLModel, Session\n",
    "\n",
    "ModelType = TypeVar(\"ModelType\", bound=SQLModel)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Database:\n",
    "    \"\"\"\n",
    "    Database class with methods to create/drop tables and add/retrieve table entries\n",
    "    \"\"\"\n",
    "    engine: Engine\n",
    "\n",
    "    @contextmanager\n",
    "    def session(self):\n",
    "        with Session(self.engine) as session:\n",
    "            yield session\n",
    "\n",
    "    def create_tables(self, tables: Optional[List[Table]] = None) -> None:\n",
    "        SQLModel.metadata.create_all(self.engine, tables=tables)\n",
    "\n",
    "    def drop_tables(self, tables: Optional[List[Table]] = None) -> None:\n",
    "        SQLModel.metadata.drop_all(self.engine, tables=tables)\n",
    "\n",
    "    def add(self, instances: List[ModelType]) -> None:\n",
    "        with self.session() as session:\n",
    "            session.add_all(instances)\n",
    "            session.commit()\n",
    "\n",
    "    def get(self, model: Type[ModelType], id: int) -> Optional[ModelType]:\n",
    "        with Session(self.engine) as session:\n",
    "            matches = session.query(model).filter(model.id == id).all()\n",
    "            if len(matches) > 1:\n",
    "                raise ValueError(\n",
    "                    f\"Multiple matches for {id=} in {model.__name__}:\\n{matches}\"\n",
    "                )\n",
    "            elif len(matches) == 0:\n",
    "                return None\n",
    "            return matches[0]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DBFactory:\n",
    "    \"\"\"\n",
    "    Factory to create Database instances\n",
    "    \"\"\"\n",
    "    engine_url: str = \"sqlite:///../database.db\"\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Database:\n",
    "        engine = create_engine(url=self.engine_url, **kwargs)\n",
    "        return Database(engine=engine, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bdf539",
   "metadata": {},
   "source": [
    "### Model and database table definitions\n",
    "Python Pydantic models and SQLite table definitions are made simultaneously using the SQLModel ORM capabilities\n",
    "\n",
    "A `RedditPost` entry will be created once when a post is fetched the first time, which includes generic metadata about the post, while a `RedditPostLogPoint` will be created periodically, which is responsible for keeping track of the score and number of comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b630ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/database_models.py\n",
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "from sqlalchemy import Column, Enum as SAEnum\n",
    "from sqlmodel import SQLModel, Field\n",
    "\n",
    "\n",
    "class GroupEnum(str, Enum):\n",
    "    CONTROL = \"CONTROL\"\n",
    "    TREATMENT = \"TREATMENT\"\n",
    "\n",
    "\n",
    "class RedditPost(SQLModel):\n",
    "    id: str = Field(primary_key=True, index=True)\n",
    "    batch_id: str = Field(\n",
    "        index=True,\n",
    "        description=\"Unique ID of the batch in which the post was added\",\n",
    "    )\n",
    "    active: bool = Field(\n",
    "        default=True, description=\"Indicates whether the post is reachable\"\n",
    "    )\n",
    "    group: GroupEnum = Field(sa_column=Column(SAEnum(GroupEnum)))\n",
    "    subreddit: str = Field()\n",
    "    title: str = Field()\n",
    "    creation_date: datetime = Field(description=\"Date at which post was created\")\n",
    "\n",
    "\n",
    "class RedditPostTable(RedditPost, table=True):\n",
    "    __tablename__ = \"RedditPost\"\n",
    "\n",
    "\n",
    "class RedditPostLogPoint(SQLModel):\n",
    "    pk: Optional[int] = Field(primary_key=True, default=None, index=True)\n",
    "    id: str = Field(index=True)\n",
    "    score: int = Field()\n",
    "    num_comments: int = Field()\n",
    "    date: datetime = Field(description=\"Date at which stats were collected\")\n",
    "\n",
    "\n",
    "class RedditPostLogPointTable(RedditPostLogPoint, table=True):\n",
    "    __tablename__ = \"RedditPostLogPoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed8ba41",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "A few utility functions are also defined which are primarily concerned with increasing the robustness of the data gathering solution.\n",
    "\n",
    "The logger and its factory method provide a structured interface for saving logs persistently to disk when the main job runs for longer periods of time without manual intervention, and the `safe_call` will prevent the process from terminating immediately on any error - e.g. if a single post out of thousands in a batch raise a 404 error because it was deleted, then we still want to process the rest of the batch. Likewise, it makes sense to retry requests if the connection temporarily drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c2a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/utils.py\n",
    "from typing import Callable, Any, List, Dict, Optional\n",
    "from time import sleep\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "from prawcore import exceptions\n",
    "\n",
    "\n",
    "def get_logger(name: str = \"RDS-PROJECT\") -> logging.Logger:\n",
    "    logger = logging.getLogger(name)\n",
    "    fhandler = logging.FileHandler(filename=\"logs.log\", mode=\"a\")\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    fhandler.setFormatter(formatter)\n",
    "    logger.addHandler(fhandler)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def safe_call(\n",
    "    func: Callable,\n",
    "    args: Optional[List] = None,\n",
    "    kwargs: Optional[Dict] = None,\n",
    "    max_retries: int = 3,\n",
    "    sleep_time: int = 1,\n",
    "    exception: Exception = exceptions.NotFound,\n",
    "    raise_on_failure: bool = True,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Wraps a function and retries it if it raises an exception.\n",
    "    \"\"\"\n",
    "    logger = get_logger()\n",
    "\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    error = Exception(\"Unknown error\")\n",
    "\n",
    "    while max_retries > 0:\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except exception as e:\n",
    "            error = e\n",
    "            max_retries -= 1\n",
    "            logger.info(\n",
    "                f\"{func.__name__} failed with args {args} and kwargs {kwargs}\\n\"\n",
    "                f\"{e}\\n{traceback.format_exc()}\"\n",
    "                f\"{max_retries} retries left\"\n",
    "            )\n",
    "            sleep(sleep_time)\n",
    "\n",
    "    if raise_on_failure:\n",
    "        logger.error(f\"Failed to execute function {func.__name__}\")\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e0f73",
   "metadata": {},
   "source": [
    "### The reddit interface\n",
    "Interfacing with reddit is done via the praw reddit API wrapper, which in turn is wrapped in the RedditBot class. This class provides methods to:\n",
    "- Fetch new posts:\n",
    "    - A random batch of new posts can be fetched with `get_batch_of_posts`\n",
    "    - This batch of posts can be randomly divided into CONTROL and TREATMENT groups with `group_posts`\n",
    "- Interface with database:\n",
    "    - Add:\n",
    "        - Posts (i.e. metadata about subreddit, creation_date, CONTROL/TREATMENT group, etc.) can be added with `add_posts_to_db`\n",
    "        - Log points (i.e. observations of score and number of comments) can be added with `add_log_points`\n",
    "    - Get:\n",
    "        - Posts no older than a certain amount of days can be retrieved from the database with `get_stored_posts`\n",
    "- The data (title, score, etc.) for a list of posts can be fetched given a list of ids with `get_posts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a39c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/reddit_bot.py\n",
    "import os\n",
    "import random\n",
    "import traceback\n",
    "from datetime import datetime, timedelta\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from uuid import uuid4\n",
    "import json\n",
    "\n",
    "import praw\n",
    "\n",
    "# Local imports suppressed in notebook cells since the objects are already available in scope.\n",
    "# from src.database import DBFactory\n",
    "# from src.database_models import RedditPostTable, RedditPostLogPointTable, GroupEnum\n",
    "# from src.utils import get_logger, safe_call\n",
    "\n",
    "CLIENT_ID = os.getenv(\"CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"CLIENT_SECRET\")\n",
    "USERNAME = os.getenv(\"USERNAME\")\n",
    "PASSWORD = os.getenv(\"PASSWORD\")\n",
    "USER_AGENT = os.getenv(\"USER_AGENT\")\n",
    "RATELIMIT = int(os.getenv(\"RATELIMIT\", 5))\n",
    "\n",
    "logger = get_logger(\"REDDIT-BOT\")\n",
    "\n",
    "\n",
    "class RedditBot:\n",
    "    \"\"\"\n",
    "    Wrapper for the Reddit bot.\n",
    "\n",
    "    It contains the following methods:\n",
    "    - get_batch_of_posts: Selects a batch of posts for the experiment\n",
    "    - group_posts: Groups a list of posts into treatment and control groups\n",
    "    - add_posts_to_db: Adds a list of posts to the database\n",
    "    - add_log_points: Adds a list of log points to the database\n",
    "    - get_stored_posts: Fetches posts from the database given a date filter\n",
    "    - get_posts: Fetches posts from the Reddit API given a list of ids\n",
    "    \"\"\"\n",
    "\n",
    "    reddit: praw.Reddit\n",
    "    url: str = \"https://www.reddit.com\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        client_id: str = CLIENT_ID,\n",
    "        client_secret: str = CLIENT_SECRET,\n",
    "        username: str = USERNAME,\n",
    "        password: str = PASSWORD,\n",
    "        user_agent: str = USER_AGENT,\n",
    "        ratelimit: int = RATELIMIT,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Authenticates the bot and initializes the Reddit instance.\n",
    "        \"\"\"\n",
    "        assert isinstance(client_id, str)\n",
    "        assert isinstance(client_secret, str)\n",
    "        assert isinstance(username, str)\n",
    "        assert isinstance(password, str)\n",
    "        assert isinstance(user_agent, str)\n",
    "        assert isinstance(ratelimit, int)\n",
    "\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret,\n",
    "            username=username,\n",
    "            password=password,\n",
    "            user_agent=user_agent,\n",
    "            ratelimit=ratelimit,\n",
    "        )\n",
    "\n",
    "    def get_batch_of_posts(\n",
    "        self,\n",
    "        subreddit: str = \"all\",\n",
    "        score: int = 1,\n",
    "        num_comments: int = 1,\n",
    "        batch_size: int = 64,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Selects a batch of posts with at most 'score' number of upvotes and\n",
    "        'num_comments' number of comments in the given subreddit.\n",
    "\n",
    "        NOTE: batch_size is an upper bound on the number of posts returned.\n",
    "        \"\"\"\n",
    "\n",
    "        posts = [\n",
    "            post\n",
    "            for post in self.reddit.subreddit(subreddit).new(limit=batch_size)\n",
    "            if post.score <= score and post.num_comments <= num_comments\n",
    "        ]\n",
    "\n",
    "        return posts\n",
    "\n",
    "    @staticmethod\n",
    "    def group_posts(\n",
    "        posts: List[praw.models.Submission],\n",
    "    ) -> Tuple[List[praw.models.Submission], List[praw.models.Submission]]:\n",
    "        \"\"\"\n",
    "        Assigns posts into treatment and control groups.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_id = str(uuid4())\n",
    "        random.shuffle(posts)\n",
    "        if len(posts) % 2 != 0:\n",
    "            posts.pop()  # Drop a random post to make the list even\n",
    "\n",
    "        middle = len(posts) // 2\n",
    "        treatment_posts = posts[:middle]\n",
    "        control_posts = posts[middle:]\n",
    "\n",
    "        for post in treatment_posts:\n",
    "            post.upvote()\n",
    "            post.group = GroupEnum.TREATMENT\n",
    "            post.batch_id = batch_id\n",
    "\n",
    "        for post in control_posts:\n",
    "            post.group = GroupEnum.CONTROL\n",
    "            post.batch_id = batch_id\n",
    "\n",
    "        return treatment_posts, control_posts\n",
    "\n",
    "    @staticmethod\n",
    "    def add_posts_to_db(\n",
    "        posts: List[praw.models.Submission],\n",
    "        backup: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "        prepared_posts = []\n",
    "\n",
    "        def _prepare_post(post: praw.models.Submission) -> Dict:\n",
    "            return dict(\n",
    "                id=post.id,\n",
    "                batch_id=post.batch_id,\n",
    "                group=post.group,\n",
    "                subreddit=str(post.subreddit),\n",
    "                title=post.title,\n",
    "                creation_date=post.created_utc,\n",
    "            )\n",
    "\n",
    "        for post in posts:\n",
    "            prepared_post = safe_call(\n",
    "                _prepare_post,\n",
    "                args=[post],\n",
    "                exception=Exception,\n",
    "                raise_on_failure=False,\n",
    "            )\n",
    "            if prepared_post is not None:\n",
    "                prepared_posts.append(prepared_post)\n",
    "\n",
    "        if backup:\n",
    "            today = datetime.today().date().isoformat()\n",
    "            with open(f\"backup/REDDITBOT_{today}_{str(uuid4())}.json\", \"w\") as f:\n",
    "                json.dump(prepared_posts, f, indent=4, default=str)\n",
    "\n",
    "        db = DBFactory()()\n",
    "\n",
    "        parsed_posts = [RedditPostTable(**post) for post in prepared_posts]\n",
    "        db.add(parsed_posts)\n",
    "        logger.info(f\"Added {len(parsed_posts)} posts to the database\")\n",
    "\n",
    "        RedditBot.add_log_points(posts, backup=backup)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_log_points(\n",
    "        posts: List[praw.models.Submission], backup: bool = False\n",
    "    ) -> None:\n",
    "\n",
    "        prepared_posts = []\n",
    "        stale_posts = []\n",
    "\n",
    "        def _prepare_post(post: praw.models.Submission) -> Dict:\n",
    "            return dict(\n",
    "                id=post.id,\n",
    "                score=post.score,\n",
    "                num_comments=post.num_comments,\n",
    "                date=datetime.now(),\n",
    "            )\n",
    "\n",
    "        for post in posts:\n",
    "            prepared_post = safe_call(\n",
    "                _prepare_post,\n",
    "                args=[post],\n",
    "                exception=Exception,\n",
    "                raise_on_failure=False,\n",
    "            )\n",
    "            if prepared_post is not None:\n",
    "                prepared_posts.append(prepared_post)\n",
    "            else:\n",
    "                stale_posts.append(post)\n",
    "\n",
    "        if backup:\n",
    "            today = datetime.today().date().isoformat()\n",
    "            with open(f\"backup/REDDITBOT_{today}_{str(uuid4())}.json\", \"w\") as f:\n",
    "                json.dump(prepared_posts, f, indent=4, default=str)\n",
    "\n",
    "        db = DBFactory()()\n",
    "\n",
    "        parsed_posts = [RedditPostLogPointTable(**post) for post in prepared_posts]\n",
    "        db.add(parsed_posts)\n",
    "        logger.info(f\"Added {len(parsed_posts)} log points to database\")\n",
    "\n",
    "        for post in stale_posts:\n",
    "            old_instance = db.get(RedditPostTable, id=post.id)\n",
    "            if old_instance is not None:\n",
    "                old_instance.active = False\n",
    "                db.add([old_instance])\n",
    "\n",
    "        logger.info(f\"Marked {len(stale_posts)} stale posts\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_stored_posts(max_age: int = 8) -> List[RedditPostTable]:\n",
    "        db = DBFactory()()\n",
    "\n",
    "        with db.session() as session:\n",
    "            posts = (\n",
    "                session.query(RedditPostTable)\n",
    "                .filter(\n",
    "                    RedditPostTable.creation_date\n",
    "                    >= (datetime.now() - timedelta(days=max_age))\n",
    "                )\n",
    "                .filter(RedditPostTable.active)\n",
    "                .all()\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Fetched {len(posts)} active posts from the database\")\n",
    "\n",
    "            return posts\n",
    "\n",
    "    def _submission_wrapper(self, *args, **kwargs) -> Optional[praw.models.Submission]:\n",
    "        try:\n",
    "            return self.reddit.submission(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                f\"{e}\\n{traceback.format_exc()}\\nargs: {args}\\nkwargs: {kwargs}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    def get_posts(\n",
    "        self, ids: List[str], threads: int = 4\n",
    "    ) -> List[praw.models.Submission]:\n",
    "        with ThreadPool(threads) as pool:\n",
    "            posts = pool.map(self._submission_wrapper, ids)\n",
    "\n",
    "        posts = [post for post in posts if post is not None]\n",
    "\n",
    "        return posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3d40ed",
   "metadata": {},
   "source": [
    "### One-time entrypoint - `setup.py`\n",
    "The `setup` function simply establishes a connection to- and possibly creates the database, after which it drops any existing tables and creates new ones afresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32395a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup.py\n",
    "\n",
    "# Local imports suppressed in notebook cells since the objects are already available in scope.\n",
    "# from src.database_models import RedditPostTable, RedditPostLogPointTable  # NOQA : F401\n",
    "# from src.database import DBFactory\n",
    "\n",
    "\n",
    "def setup():\n",
    "    db = DBFactory()()\n",
    "    db.drop_tables()\n",
    "    db.create_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f43d3e",
   "metadata": {},
   "source": [
    "### Main entrypoint - `main.py`\n",
    "With everything set up, the `main` function defines a simple routine for monitoring the stats of previously fetched posts as well as fetching a batch of newly created posts. This is the function that is deployed to run periodically. Check [the GitHub repo](https://github.com/NValsted/RDS-Project-2022-1) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af34d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "# Local imports suppressed in notebook cells since the objects are already available in scope.\n",
    "# from src import RedditBot\n",
    "# from src.utils import safe_call\n",
    "\n",
    "\n",
    "def main():\n",
    "    bot = RedditBot()\n",
    "\n",
    "    # Update old Posts\n",
    "    posts = bot.get_stored_posts()\n",
    "    ids = {post.id for post in posts}\n",
    "\n",
    "    posts = bot.get_posts(ids)\n",
    "    bot.add_log_points(posts)\n",
    "\n",
    "    # New posts\n",
    "    treatment, control = safe_call(\n",
    "        func=lambda: bot.group_posts(bot.get_batch_of_posts())\n",
    "    )\n",
    "    bot.add_posts_to_db(treatment)\n",
    "    bot.add_posts_to_db(control)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27102d",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "At this point, roughly 40000 posts have been fetched and monitored over the course of 7 days each, which has resulted in around 2.5 million log points. This section marks the start of an analysis of the resulting data.\n",
    "\n",
    "A snapshot of the database is available at https://ituniversity-my.sharepoint.com/:u:/g/personal/nicv_itu_dk/EUPPi364WC1Am9hZAJ__3ScByk-dMOuZBrM0VteAbQjhYw?e=lkJ3Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f647ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df1c96",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d19ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_post_df = pd.read_sql_table(RedditPostTable.__tablename__, DBFactory.engine_url)\n",
    "reddit_post_log_point_df = pd.read_sql_table(RedditPostLogPointTable.__tablename__, DBFactory.engine_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94eecc",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Before commencing with the analysis, a little preprocessing is beneficial, e.g. due to the fact that certain posts are marked inactive since they have been deleted or otherwise made unreachable, which has unbalanced the dataset slightly.\n",
    "\n",
    "The preprocessing steps are the following (Which are intertwined in practice for convenience):\n",
    "- Join post info and log points (`RedditPostTable` and `RedditPostLogPointTable`)\n",
    "- Balance dataset. \n",
    "    - For each inactive post, identify a post from the conjugate group with the same `batch_id` and filter away both.\n",
    "    - Filter away any post that has not been monitored for at least 7 days (i.e. younger than 7 days or marked inactive before the 7 day mark).\n",
    "- Create derived columns: `age` and `saturation`.\n",
    "- Unbias treatment group by subtracting 1 from the score\n",
    "- For each of the two groups, create dataframes containing only the latest log point for a post.\n",
    "\n",
    "These steps will be described further in the relevant cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f326aeda",
   "metadata": {},
   "source": [
    "### Join dataframes\n",
    "A join between the RedditPost and RedditPostLogPoint on the `id` column is done (reddit's id for a given post), which essentially yields RedditPostLogPoint but with all the relevant metadata attached for the post which the log point corresponds to. A random sample of the dataframe is shown as an example of the resulting format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d6fea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = pd.merge(reddit_post_df, reddit_post_log_point_df, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96e67e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>id</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>active</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>pk</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1758912</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>u7kha3</td>\n",
       "      <td>58e477e3-afd6-40c8-9ec5-541c02e697b1</td>\n",
       "      <td>True</td>\n",
       "      <td>kucoin</td>\n",
       "      <td>Most Crypto friendly country</td>\n",
       "      <td>2022-04-20 01:05:16</td>\n",
       "      <td>1887674</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2022-04-28 00:32:10.161076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855411</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>tjzl5k</td>\n",
       "      <td>45f07b5f-e7e8-448c-b5ec-58b1436b7471</td>\n",
       "      <td>True</td>\n",
       "      <td>granturismo</td>\n",
       "      <td>Wow, they really did make the car prices reali...</td>\n",
       "      <td>2022-03-22 10:06:22</td>\n",
       "      <td>765566</td>\n",
       "      <td>731</td>\n",
       "      <td>61</td>\n",
       "      <td>2022-03-23 15:39:51.274643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846567</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>tjugzb</td>\n",
       "      <td>e038c323-06b2-4337-8dba-c32a70d8558d</td>\n",
       "      <td>True</td>\n",
       "      <td>Amillennialism</td>\n",
       "      <td>lol wut</td>\n",
       "      <td>2022-03-22 04:06:17</td>\n",
       "      <td>907084</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-03-28 00:24:20.887211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031129</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>udrkme</td>\n",
       "      <td>d2441cd4-a212-4d40-8286-7f3cbe58d9ca</td>\n",
       "      <td>True</td>\n",
       "      <td>CryptoHatcher</td>\n",
       "      <td>Using Binance</td>\n",
       "      <td>2022-04-28 10:06:08</td>\n",
       "      <td>1998821</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-05-01 12:15:33.878562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454359</th>\n",
       "      <td>TREATMENT</td>\n",
       "      <td>taqe69</td>\n",
       "      <td>3b0a1c05-2687-461e-88f3-01cab341c760</td>\n",
       "      <td>True</td>\n",
       "      <td>TeamfightTactics</td>\n",
       "      <td>Who needs 3 Magnetic Removals??</td>\n",
       "      <td>2022-03-10 04:24:14</td>\n",
       "      <td>259732</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>2022-03-10 19:20:12.294425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             group      id                              batch_id  active  \\\n",
       "1758912    CONTROL  u7kha3  58e477e3-afd6-40c8-9ec5-541c02e697b1    True   \n",
       "855411     CONTROL  tjzl5k  45f07b5f-e7e8-448c-b5ec-58b1436b7471    True   \n",
       "846567     CONTROL  tjugzb  e038c323-06b2-4337-8dba-c32a70d8558d    True   \n",
       "2031129    CONTROL  udrkme  d2441cd4-a212-4d40-8286-7f3cbe58d9ca    True   \n",
       "454359   TREATMENT  taqe69  3b0a1c05-2687-461e-88f3-01cab341c760    True   \n",
       "\n",
       "                subreddit                                              title  \\\n",
       "1758912            kucoin                       Most Crypto friendly country   \n",
       "855411        granturismo  Wow, they really did make the car prices reali...   \n",
       "846567     Amillennialism                                            lol wut   \n",
       "2031129     CryptoHatcher                                      Using Binance   \n",
       "454359   TeamfightTactics                    Who needs 3 Magnetic Removals??   \n",
       "\n",
       "              creation_date       pk  score  num_comments  \\\n",
       "1758912 2022-04-20 01:05:16  1887674      2            10   \n",
       "855411  2022-03-22 10:06:22   765566    731            61   \n",
       "846567  2022-03-22 04:06:17   907084      4             1   \n",
       "2031129 2022-04-28 10:06:08  1998821      1             0   \n",
       "454359  2022-03-10 04:24:14   259732     17             9   \n",
       "\n",
       "                              date  \n",
       "1758912 2022-04-28 00:32:10.161076  \n",
       "855411  2022-03-23 15:39:51.274643  \n",
       "846567  2022-03-28 00:24:20.887211  \n",
       "2031129 2022-05-01 12:15:33.878562  \n",
       "454359  2022-03-10 19:20:12.294425  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b9e7db",
   "metadata": {},
   "source": [
    "### Derived columns p0 - age\n",
    "The `age` column simply denotes how long it has been since a post was created when a log point was recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7bb0106",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined[\"age\"] = joined[\"date\"] - joined[\"creation_date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55acf5",
   "metadata": {},
   "source": [
    "### Unbias TREATMENT group\n",
    "The initial upvote of 1 is subtracted from the TREATMENT group, since we are interested in investigating whether the treatment has increased popularity as expressed by external users - i.e. all users that are not our bot.\n",
    "\n",
    "Perhaps, this is best explained with an example in the extremes: If we assume an artificial world where no other users interact with posts on the platform, then we can safely say beforehand that the treatment will not have an effect. If we then perform our experiment, all posts in the CONTROL group will have a score of 0, and all posts in the TREATMENT group will have a score of 1. If we were to perform any analysis on the resulting data without unbiasing the TREATMENT group, we would wrongfully conclude effectiveness of the TREATMENT due to the difference in score distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdc68433",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.loc[joined[\"group\"] == GroupEnum.TREATMENT.value, (\"score\")] -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d0a1b4",
   "metadata": {},
   "source": [
    "### Latest posts\n",
    "We have record many log points for each individual post, so the `joined_latest` dataframe consists of only the latest log point for each post  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "484b3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_latest = joined.sort_values([\"date\"], ascending=False).groupby(by=\"id\", as_index=False).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f36d7d",
   "metadata": {},
   "source": [
    "### Balance dataset\n",
    "To balance the dataset, we want remove any posts that have not yet been tracked for at least 7 days, and we also want to ensure that there are equally many data points in the control and treatment groups. Since we sample an equal amount of control and treatment posts in each batch, the only reason these can be different is if one or more posts have been marked inactive (i.e. deleted or otherwise unreachable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "034c6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_drop = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa300b",
   "metadata": {},
   "source": [
    "First, filter posts younger than 7 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6443046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in joined_latest[joined_latest[\"age\"] < timedelta(days=7)].iterrows():\n",
    "    ids_to_drop.add(str(row[\"id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6644a7f",
   "metadata": {},
   "source": [
    "Then balance pairs within batches, prioritizing removing conjugate posts which are also inactive - e.g. if a batch contains a single inactive control post and inactive treatment post, then these two should simply cancel out. Otherwise, simply sample a random active post of the conjugate group to cancel out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf365747",
   "metadata": {},
   "outputs": [],
   "source": [
    "inactive_posts = joined_latest[joined_latest[\"active\"] == False]\n",
    "\n",
    "for group in inactive_posts.groupby(by=\"batch_id\", as_index=False):\n",
    "    group_key, group_df = group\n",
    "\n",
    "    control_remainder, treatment_remainder = (\n",
    "        joined_latest[\n",
    "            (joined_latest[\"batch_id\"] == group_key)\n",
    "            & (joined_latest[\"group\"] == group.value)\n",
    "            & ~(joined_latest[\"id\"].isin(set(group_df[\"id\"])))\n",
    "        ]\n",
    "        for group in (GroupEnum.CONTROL, GroupEnum.TREATMENT)\n",
    "    )\n",
    "    num_control = control_remainder.shape[0]\n",
    "    num_treatment = treatment_remainder.shape[0]\n",
    "    \n",
    "    if num_control != num_treatment:\n",
    "        conjugate_remainder = control_remainder if num_treatment < num_control else treatment_remainder\n",
    "        num_to_drop = abs(num_control - num_treatment)\n",
    "        to_drop = conjugate_remainder.sample(n=num_to_drop)\n",
    "        ids_to_drop.add(str(to_drop[\"id\"]))\n",
    "        \n",
    "    for entry in group_df[\"id\"]:\n",
    "        ids_to_drop.add(str(entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1663f278",
   "metadata": {},
   "source": [
    "Apply filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "869a228f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 4335 post(s)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dropping {len(ids_to_drop)} post(s)\")\n",
    "\n",
    "joined_latest = joined_latest[~(joined_latest[\"id\"].isin(ids_to_drop))]\n",
    "# joined = joined[~(joined[\"id\"].isin(ids_to_drop))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1ecf4",
   "metadata": {},
   "source": [
    "### Derived columns p1 - saturation\n",
    "Saturation is useful in a temporal context and describes the ratio of some maximum value for a post. E.g. a post with `score` values 0, 50, and 100 will be mapped to `score_saturation` values of 0, 0.5, 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ac30f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined[\"score_saturation\"] = joined[\"score\"] / joined.groupby(\"id\")[\"score\"].transform(np.max)\n",
    "joined[\"num_comments_saturation\"] = joined[\"num_comments\"] / joined.groupby(\"id\")[\"num_comments\"].transform(np.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ede8aa",
   "metadata": {},
   "source": [
    "### Overview of resulting dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4771b2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>id</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>active</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>pk</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>date</th>\n",
       "      <th>age</th>\n",
       "      <th>score_saturation</th>\n",
       "      <th>num_comments_saturation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1467909</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>u0s8et</td>\n",
       "      <td>1bc51938-e0e3-4e64-809c-0359e152a540</td>\n",
       "      <td>True</td>\n",
       "      <td>pokemongo</td>\n",
       "      <td>crazy community day here with the new incense ...</td>\n",
       "      <td>2022-04-10 22:03:08</td>\n",
       "      <td>1462574</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>2022-04-14 18:14:22.385070</td>\n",
       "      <td>3 days 20:11:14.385070</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593025</th>\n",
       "      <td>TREATMENT</td>\n",
       "      <td>tdumdy</td>\n",
       "      <td>d83ebc33-8e43-42af-b9ef-a3a79061cc52</td>\n",
       "      <td>True</td>\n",
       "      <td>SoundCloudHipHop</td>\n",
       "      <td>[FRESH] KreamDaReap - ReapInTheFlesh (Prod.Xantu)</td>\n",
       "      <td>2022-03-14 10:43:42</td>\n",
       "      <td>630288</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-03-19 12:25:19.106356</td>\n",
       "      <td>5 days 01:41:37.106356</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546293</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>tcsw69</td>\n",
       "      <td>76c78e4e-702f-46d6-8951-db40397da591</td>\n",
       "      <td>True</td>\n",
       "      <td>DMAcademy</td>\n",
       "      <td>How to run wilderness \"chase\" (party being tra...</td>\n",
       "      <td>2022-03-12 22:46:51</td>\n",
       "      <td>607401</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-03-18 18:50:57.080665</td>\n",
       "      <td>5 days 20:04:06.080665</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870640</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>tkei94</td>\n",
       "      <td>a563bfdb-8464-484a-975a-0132136fffbe</td>\n",
       "      <td>True</td>\n",
       "      <td>theisle</td>\n",
       "      <td>Tenno what the hell dood?</td>\n",
       "      <td>2022-03-22 22:07:23</td>\n",
       "      <td>803955</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-03-24 21:02:08.858438</td>\n",
       "      <td>1 days 22:54:45.858438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734283</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>th4tk1</td>\n",
       "      <td>8fbc5db6-87db-4636-bc62-2f8c64afded8</td>\n",
       "      <td>True</td>\n",
       "      <td>SeikoMods</td>\n",
       "      <td>1.7mm Allan Key Needed</td>\n",
       "      <td>2022-03-18 16:06:21</td>\n",
       "      <td>683888</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2022-03-21 03:35:17.046362</td>\n",
       "      <td>2 days 11:28:56.046362</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             group      id                              batch_id  active  \\\n",
       "1467909    CONTROL  u0s8et  1bc51938-e0e3-4e64-809c-0359e152a540    True   \n",
       "593025   TREATMENT  tdumdy  d83ebc33-8e43-42af-b9ef-a3a79061cc52    True   \n",
       "546293     CONTROL  tcsw69  76c78e4e-702f-46d6-8951-db40397da591    True   \n",
       "870640     CONTROL  tkei94  a563bfdb-8464-484a-975a-0132136fffbe    True   \n",
       "734283     CONTROL  th4tk1  8fbc5db6-87db-4636-bc62-2f8c64afded8    True   \n",
       "\n",
       "                subreddit                                              title  \\\n",
       "1467909         pokemongo  crazy community day here with the new incense ...   \n",
       "593025   SoundCloudHipHop  [FRESH] KreamDaReap - ReapInTheFlesh (Prod.Xantu)   \n",
       "546293          DMAcademy  How to run wilderness \"chase\" (party being tra...   \n",
       "870640            theisle                          Tenno what the hell dood?   \n",
       "734283          SeikoMods                             1.7mm Allan Key Needed   \n",
       "\n",
       "              creation_date       pk  score  num_comments  \\\n",
       "1467909 2022-04-10 22:03:08  1462574     12             7   \n",
       "593025  2022-03-14 10:43:42   630288      2             1   \n",
       "546293  2022-03-12 22:46:51   607401      1             5   \n",
       "870640  2022-03-22 22:07:23   803955      2             2   \n",
       "734283  2022-03-18 16:06:21   683888      1            11   \n",
       "\n",
       "                              date                    age  score_saturation  \\\n",
       "1467909 2022-04-14 18:14:22.385070 3 days 20:11:14.385070          0.857143   \n",
       "593025  2022-03-19 12:25:19.106356 5 days 01:41:37.106356          1.000000   \n",
       "546293  2022-03-18 18:50:57.080665 5 days 20:04:06.080665          1.000000   \n",
       "870640  2022-03-24 21:02:08.858438 1 days 22:54:45.858438          1.000000   \n",
       "734283  2022-03-21 03:35:17.046362 2 days 11:28:56.046362          1.000000   \n",
       "\n",
       "         num_comments_saturation  \n",
       "1467909                      1.0  \n",
       "593025                       1.0  \n",
       "546293                       1.0  \n",
       "870640                       1.0  \n",
       "734283                       1.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38ac38",
   "metadata": {},
   "source": [
    "### Split dataframe in groups\n",
    "Finally, we simply create dataframes containing only values from the respective groups for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e906a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "control, treatment = (joined[joined[\"group\"] == group.value] for group in (GroupEnum.CONTROL, GroupEnum.TREATMENT))\n",
    "control_latest, treatment_latest = (\n",
    "    joined_latest[joined_latest[\"group\"] == group.value] for group in (GroupEnum.CONTROL, GroupEnum.TREATMENT)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0545e336",
   "metadata": {},
   "source": [
    "## Distribution similarity\n",
    "The following section investigates the similarity between the score and number of comments distributions between the two groups, in order to evaluate the effectiveness of the treatment.\n",
    "\n",
    "The following plot displays histograms of the data points within each group. The general distribution type seems similar, but parameters seem to be noticeably different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86a2c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import io as pio, graph_objects as go, express as px\n",
    "from plotly.subplots import make_subplots\n",
    "pio.renderers.default = \"iframe\"\n",
    "# Jupyter notebooks don't handle plotly figures well.\n",
    "# Therefore, iframes and %%capture are used to save the resulting html files to disk instead.\n",
    "# These should appear in the iframe_figures directory, following the figure_{cell_number}.html naming scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56afc4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    shared_yaxes=True,\n",
    "    subplot_titles=(\"Score distribution\", \"Number of comments distribution\"),\n",
    "    horizontal_spacing=0.05,\n",
    ")\n",
    "\n",
    "for i in (1, 2):\n",
    "    for j, attr in ((1, \"score\"), (2, \"num_comments\")):\n",
    "        for df, color in ((control_latest, \"#4969AA\"), (treatment_latest, \"#C90D0D\")):\n",
    "            fig.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=df[attr],\n",
    "                    histnorm=\"percent\",\n",
    "                    name=df[\"group\"].min(),\n",
    "                    xbins=dict(\n",
    "                        start=df[attr].min(),\n",
    "                        end=df[attr].max(),\n",
    "                        size=0.5\n",
    "                    ),\n",
    "                    marker_color=color,\n",
    "                    opacity=0.75,\n",
    "                    showlegend=i == 1 and j == 1\n",
    "                ),\n",
    "                row=i,\n",
    "                col=j,\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            yaxis_title_text=\"Percent\",\n",
    "            bargap=0.2,\n",
    "            bargroupgap=0.1\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(type=\"log\", row=1, col=1)\n",
    "fig.update_xaxes(type=\"log\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Score (log)\", row=2, col=1, type=\"log\")\n",
    "fig.update_xaxes(title_text=\"Number of comments (log)\", row=2, col=2, type=\"log\")\n",
    "\n",
    "fig.update_yaxes(title_text=\"Percent (log)\", type=\"log\", row=2, col=1)\n",
    "fig.update_yaxes(type=\"log\", row=2, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b615d28c",
   "metadata": {},
   "source": [
    "### Kolmogorov-Smirnov test\n",
    "With the Kolmogorov-Smirnov test, we perform an empirical test to investigate whether the data points appear to belong to the same distribution. The null hypothesis is that the distributions are identical, and thus a low p-value indicates that there is a significant difference between the control and treatment groups. As can be seen below, the results are in favour of the alternative hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00dafe30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KstestResult(statistic=0.050203606748109364, pvalue=3.0660866043136154e-19)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.kstest(control_latest[\"score\"], treatment_latest[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "261bf17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KstestResult(statistic=0.03378460953297813, pvalue=6.0263840562854085e-09)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.kstest(control_latest[\"num_comments\"], treatment_latest[\"num_comments\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc95ae8",
   "metadata": {},
   "source": [
    "However, small fluctuations might provide an inaccurate picture - e.g. a post with a score of 34 is probably not significantly different from one with a score of 35. So we will investigate whether rounding the score and number of comments to the nearest multiple of a range of numbers will jeopardize the result above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cc1c7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 22.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 22.14it/s]\n"
     ]
    }
   ],
   "source": [
    "binned_ks_test_score = [stats.kstest(*(df[\"score\"].map(lambda x: i * round(x / i)) for df in (control_latest, treatment_latest))).pvalue for i in tqdm(range(1, 10))]\n",
    "binned_ks_test_num_comments = [stats.kstest(*(df[\"num_comments\"].map(lambda x: i * round(x / i)) for df in (control_latest, treatment_latest))).pvalue for i in tqdm(range(1, 10))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c839c",
   "metadata": {},
   "source": [
    "...But as the plot below shows, even pessimistically trying several different bin sizes, the maximum p-value is still no larger than on the order of $10^{-8}$, which occurs for the score distributions when rounding the score to the nearest multiple of 5. Thus, it seems more likely that there is a significant difference between the control and treatment groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dcc671c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_26.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    subplot_titles=(\"KS-test binned score distributions\", \"KS-test binned number of comments distributions\"),\n",
    ")\n",
    "for i, lst in enumerate((binned_ks_test_score, binned_ks_test_num_comments)):\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, len(lst) + 1)), y=lst), row=i+1, col=1)\n",
    "fig.update_yaxes(type=\"log\", row=\"all\", col=\"all\")\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3fce1",
   "metadata": {},
   "source": [
    "### Descriptive statistics\n",
    "The following section simply displays key descriptive statistics for the 2 distribution pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c963f1",
   "metadata": {},
   "source": [
    "#### Quantile statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21caaffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTROL: score\n",
      "count    17142.000000\n",
      "mean        73.678684\n",
      "std        757.802228\n",
      "min          0.000000\n",
      "25%          1.000000\n",
      "50%          2.000000\n",
      "75%         14.000000\n",
      "max      43660.000000\n",
      "Name: score, dtype: float64\n",
      "\n",
      "TREATMENT: score\n",
      "count    17190.000000\n",
      "mean        80.589412\n",
      "std        939.957942\n",
      "min         -1.000000\n",
      "25%          1.000000\n",
      "50%          3.000000\n",
      "75%         16.000000\n",
      "max      52543.000000\n",
      "Name: score, dtype: float64\n",
      "\n",
      "CONTROL: num_comments\n",
      "count    17142.000000\n",
      "mean        12.400187\n",
      "std        263.191000\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          2.000000\n",
      "75%          7.000000\n",
      "max      32770.000000\n",
      "Name: num_comments, dtype: float64\n",
      "\n",
      "TREATMENT: num_comments\n",
      "count    17190.000000\n",
      "mean        11.706923\n",
      "std         76.597475\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          2.000000\n",
      "75%          8.000000\n",
      "max       6577.000000\n",
      "Name: num_comments, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for attr in (\"score\", \"num_comments\"):\n",
    "    for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest)):\n",
    "        print(f\"{name}: {attr}\")\n",
    "        print(df[attr].describe())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bcd908",
   "metadata": {},
   "source": [
    "#### Skewness and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a0b5627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRIC: skew\n",
      "CONTROL score skew: 31.450308607514575\n",
      "TREATMENT score skew: 38.79248590691322\n",
      "CONTROL num_comments skew: 114.51770761538164\n",
      "TREATMENT num_comments skew: 53.29467656908218\n",
      "\n",
      "METRIC: kurtosis\n",
      "CONTROL score kurtosis: 1311.6424276146558\n",
      "TREATMENT score kurtosis: 1865.065871456808\n",
      "CONTROL num_comments kurtosis: 14058.14667758193\n",
      "TREATMENT num_comments kurtosis: 3912.1044260950575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for metric in (\"skew\", \"kurtosis\"):\n",
    "    print(f\"METRIC: {metric}\")\n",
    "    print(\n",
    "        \"\\n\".join(\n",
    "            (\n",
    "                f\"{name} {attr} {metric}: {getattr(df[attr], metric)()}\"\n",
    "                for attr in (\"score\", \"num_comments\")\n",
    "                for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf6c0a",
   "metadata": {},
   "source": [
    "#### (score, num_comments)-correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "447d7e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTROL:\n",
      "0.09335078773647593\n",
      "\n",
      "TREATMENT:\n",
      "0.15700463173949747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest)):\n",
    "    print(f\"{name}:\")\n",
    "    print(df[\"score\"].corr(df[\"num_comments\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8588a",
   "metadata": {},
   "source": [
    "#### Filter extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19eb0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top(df: pd.DataFrame, attr: str, qt: float = 0.95):\n",
    "    return df[df[attr] <= df[attr].quantile(qt)][attr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b4d0f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTROL: score\n",
      "count    16286.000000\n",
      "mean        13.329915\n",
      "std         28.073410\n",
      "min          0.000000\n",
      "25%          1.000000\n",
      "50%          2.000000\n",
      "75%         10.000000\n",
      "max        183.000000\n",
      "Name: score, dtype: float64\n",
      "\n",
      "TREATMENT: score\n",
      "count    16331.000000\n",
      "mean        14.657216\n",
      "std         30.408935\n",
      "min         -1.000000\n",
      "25%          1.000000\n",
      "50%          2.000000\n",
      "75%         12.000000\n",
      "max        201.000000\n",
      "Name: score, dtype: float64\n",
      "\n",
      "CONTROL: num_comments\n",
      "count    16301.000000\n",
      "mean         4.550028\n",
      "std          6.644784\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          2.000000\n",
      "75%          6.000000\n",
      "max         35.000000\n",
      "Name: num_comments, dtype: float64\n",
      "\n",
      "TREATMENT: num_comments\n",
      "count    16348.000000\n",
      "mean         5.264497\n",
      "std          7.666882\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          2.000000\n",
      "75%          7.000000\n",
      "max         41.000000\n",
      "Name: num_comments, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for attr in (\"score\", \"num_comments\"):\n",
    "    for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest)):\n",
    "        print(f\"{name}: {attr}\")\n",
    "        print(df[df[attr] <= df[attr].quantile(0.95)][attr].describe())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22f4b814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRIC: skew\n",
      "CONTROL score skew: 3.368455633673462\n",
      "TREATMENT score skew: 3.3582937899142324\n",
      "CONTROL num_comments skew: 2.114025515346434\n",
      "TREATMENT num_comments skew: 2.165640646130761\n",
      "\n",
      "METRIC: kurtosis\n",
      "CONTROL score kurtosis: 12.296269499843739\n",
      "TREATMENT score kurtosis: 12.336831661854085\n",
      "CONTROL num_comments kurtosis: 4.539776376481717\n",
      "TREATMENT num_comments kurtosis: 4.839831307073403\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for metric in (\"skew\", \"kurtosis\"):\n",
    "    print(f\"METRIC: {metric}\")\n",
    "    print(\n",
    "        \"\\n\".join(\n",
    "            (\n",
    "                f\"{name} {attr} {metric}: {getattr(df[df[attr] <= df[attr].quantile(0.95)][attr], metric)()}\"\n",
    "                for attr in (\"score\", \"num_comments\")\n",
    "                for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d31a1e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTROL:\n",
      "0.2924712999654716\n",
      "\n",
      "TREATMENT:\n",
      "0.28505258035361164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest)):\n",
    "    print(f\"{name}:\")\n",
    "    print(filter_top(df, \"score\").corr(filter_top(df, \"num_comments\")))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa436805",
   "metadata": {},
   "source": [
    "## Temporal similarity\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2b0ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "px.scatter(joined, x=\"age\", y=\"score_saturation\", color=\"group\", opacity=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "514ff875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8195551703033446"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control[\"score_saturation\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7218cad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment[\"score_saturation\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b47481a",
   "metadata": {},
   "source": [
    "# Draft conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d96ffa",
   "metadata": {},
   "source": [
    "In conclusion: From the KS-tests we deduce that the probability of the samples from the two distribution pairs - (TREATMENT_score, CONTROL_score) and (TREATMENT_num_comments, CONTROL_num_comments) - originating from the respective same distribution are both <i>significantly</i> less than 5% (p-value < 0.05). Investigating descriptive statistics of these distributions leads us to believe that the popularity of a post in terms of its score is indeed higher for posts in the treatment group. However, the opposite is true for the number of comments. Initially, we had hypothesized that score and number of comments were correlated, which does not seem to be the case, and this does tie together with the aforementioned uncorrelated effectiveness of the treatment on score and comments. However, a word of caution: The distributions are severely heavy-tailed as indicated by the kurtosis measures, and thus the extreme sample values can easily impact the results significantly, given the fairly limited size of the sample. This is demonstrated by filtering away the top 5% of the data from each distribution, yielding noticeably different perceived relationships between the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab1921f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
