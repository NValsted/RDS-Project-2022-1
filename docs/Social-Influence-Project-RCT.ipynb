{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb9df05",
   "metadata": {},
   "source": [
    "# Social Influence Project - Reddit Submission Popularity RCT <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "\n",
    "This notebook concretely illustrates the data gathering and analysis for which the main paper is based upon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fbe8a2",
   "metadata": {},
   "source": [
    "# Table of contents:\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759aabd",
   "metadata": {},
   "source": [
    "# Data gathering setup and pipeline\n",
    "\n",
    "The following section contains all the relevant python code used to interface with reddit and store the resulting data in a local SQLite database. Keep in mind that the code is not meant to be deployed and run from within a jupyter notebook. Instead, it was designed such that the entrypoint `main.py` can be run on a schedule with a cronjob. For a greater viewing experience and better overview, it is recommended to visit [the GitHub repo](https://github.com/NValsted/RDS-Project-2022-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf96405f",
   "metadata": {},
   "source": [
    "### Database interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cdb0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#src/database.py\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, TypeVar, List, Type\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.sql.schema import Table\n",
    "from sqlmodel import create_engine, SQLModel, Session\n",
    "\n",
    "ModelType = TypeVar(\"ModelType\", bound=SQLModel)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Database:\n",
    "    \"\"\"\n",
    "    Database class with methods to create/drop tables and add/retrieve table entries\n",
    "    \"\"\"\n",
    "    engine: Engine\n",
    "\n",
    "    @contextmanager\n",
    "    def session(self):\n",
    "        with Session(self.engine) as session:\n",
    "            yield session\n",
    "\n",
    "    def create_tables(self, tables: Optional[List[Table]] = None) -> None:\n",
    "        SQLModel.metadata.create_all(self.engine, tables=tables)\n",
    "\n",
    "    def drop_tables(self, tables: Optional[List[Table]] = None) -> None:\n",
    "        SQLModel.metadata.drop_all(self.engine, tables=tables)\n",
    "\n",
    "    def add(self, instances: List[ModelType]) -> None:\n",
    "        with self.session() as session:\n",
    "            session.add_all(instances)\n",
    "            session.commit()\n",
    "\n",
    "    def get(self, model: Type[ModelType], id: int) -> Optional[ModelType]:\n",
    "        with Session(self.engine) as session:\n",
    "            matches = session.query(model).filter(model.id == id).all()\n",
    "            if len(matches) > 1:\n",
    "                raise ValueError(\n",
    "                    f\"Multiple matches for {id=} in {model.__name__}:\\n{matches}\"\n",
    "                )\n",
    "            elif len(matches) == 0:\n",
    "                return None\n",
    "            return matches[0]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DBFactory:\n",
    "    \"\"\"\n",
    "    Factory to create Database instances\n",
    "    \"\"\"\n",
    "    engine_url: str = \"sqlite:///../database.db\"\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Database:\n",
    "        engine = create_engine(url=self.engine_url, **kwargs)\n",
    "        return Database(engine=engine, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bdf539",
   "metadata": {},
   "source": [
    "### Model and database table definitions\n",
    "Python Pydantic models and SQLite table definitions are made simultaneously using the SQLModel ORM capabilities\n",
    "\n",
    "A `RedditPost` entry will be created once when a post is fetched the first time, which includes generic metadata about the post, while a `RedditPostLogPoint` will be created periodically, which is responsible for keeping track of the score and number of comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b630ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/database_models.py\n",
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "from sqlalchemy import Column, Enum as SAEnum\n",
    "from sqlmodel import SQLModel, Field\n",
    "\n",
    "\n",
    "class GroupEnum(str, Enum):\n",
    "    CONTROL = \"CONTROL\"\n",
    "    TREATMENT = \"TREATMENT\"\n",
    "\n",
    "\n",
    "class RedditPost(SQLModel):\n",
    "    id: str = Field(primary_key=True, index=True)\n",
    "    batch_id: str = Field(\n",
    "        index=True,\n",
    "        description=\"Unique ID of the batch in which the post was added\",\n",
    "    )\n",
    "    active: bool = Field(\n",
    "        default=True, description=\"Indicates whether the post is reachable\"\n",
    "    )\n",
    "    group: GroupEnum = Field(sa_column=Column(SAEnum(GroupEnum)))\n",
    "    subreddit: str = Field()\n",
    "    title: str = Field()\n",
    "    creation_date: datetime = Field(description=\"Date at which post was created\")\n",
    "\n",
    "\n",
    "class RedditPostTable(RedditPost, table=True):\n",
    "    __tablename__ = \"RedditPost\"\n",
    "\n",
    "\n",
    "class RedditPostLogPoint(SQLModel):\n",
    "    pk: Optional[int] = Field(primary_key=True, default=None, index=True)\n",
    "    id: str = Field(index=True)\n",
    "    score: int = Field()\n",
    "    num_comments: int = Field()\n",
    "    date: datetime = Field(description=\"Date at which stats were collected\")\n",
    "\n",
    "\n",
    "class RedditPostLogPointTable(RedditPostLogPoint, table=True):\n",
    "    __tablename__ = \"RedditPostLogPoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed8ba41",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "A few utility functions are also defined which are primarily concerned with increasing the robustness of the data gathering solution.\n",
    "\n",
    "The logger and its factory method provide a structured interface for saving logs persistently to disk when the main job runs for longer periods of time without manual intervention, and the `safe_call` will prevent the process from terminating immediately on any error - e.g. if a single post out of thousands in a batch raise a 404 error because it was deleted, then we still want to process the rest of the batch. Likewise, it makes sense to retry requests if the connection temporarily drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c2a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/utils.py\n",
    "from typing import Callable, Any, List, Dict, Optional\n",
    "from time import sleep\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "from prawcore import exceptions\n",
    "\n",
    "\n",
    "def get_logger(name: str = \"RDS-PROJECT\") -> logging.Logger:\n",
    "    logger = logging.getLogger(name)\n",
    "    fhandler = logging.FileHandler(filename=\"logs.log\", mode=\"a\")\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    fhandler.setFormatter(formatter)\n",
    "    logger.addHandler(fhandler)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def safe_call(\n",
    "    func: Callable,\n",
    "    args: Optional[List] = None,\n",
    "    kwargs: Optional[Dict] = None,\n",
    "    max_retries: int = 3,\n",
    "    sleep_time: int = 1,\n",
    "    exception: Exception = exceptions.NotFound,\n",
    "    raise_on_failure: bool = True,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Wraps a function and retries it if it raises an exception.\n",
    "    \"\"\"\n",
    "    logger = get_logger()\n",
    "\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    error = Exception(\"Unknown error\")\n",
    "\n",
    "    while max_retries > 0:\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except exception as e:\n",
    "            error = e\n",
    "            max_retries -= 1\n",
    "            logger.info(\n",
    "                f\"{func.__name__} failed with args {args} and kwargs {kwargs}\\n\"\n",
    "                f\"{e}\\n{traceback.format_exc()}\"\n",
    "                f\"{max_retries} retries left\"\n",
    "            )\n",
    "            sleep(sleep_time)\n",
    "\n",
    "    if raise_on_failure:\n",
    "        logger.error(f\"Failed to execute function {func.__name__}\")\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e0f73",
   "metadata": {},
   "source": [
    "### The reddit interface\n",
    "Interfacing with reddit is done via the praw reddit API wrapper, which in turn is wrapped in the RedditBot class. This class provides methods to:\n",
    "- Fetch new posts:\n",
    "    - A random batch of new posts can be fetched with `get_batch_of_posts`\n",
    "    - This batch of posts can be randomly divided into CONTROL and TREATMENT groups with `group_posts`\n",
    "- Interface with database:\n",
    "    - Add:\n",
    "        - Posts (i.e. metadata about subreddit, creation_date, CONTROL/TREATMENT group, etc.) can be added with `add_posts_to_db`\n",
    "        - Log points (i.e. observations of score and number of comments) can be added with `add_log_points`\n",
    "    - Get:\n",
    "        - Posts no older than a certain amount of days can be retrieved from the database with `get_stored_posts`\n",
    "- The data (title, score, etc.) for a list of posts can be fetched given a list of ids with `get_posts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a39c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/reddit_bot.py\n",
    "import os\n",
    "import random\n",
    "import traceback\n",
    "from datetime import datetime, timedelta\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from uuid import uuid4\n",
    "import json\n",
    "\n",
    "import praw\n",
    "\n",
    "# Local imports suppressed in notebook cells since the objects are already available in scope.\n",
    "# from src.database import DBFactory\n",
    "# from src.database_models import RedditPostTable, RedditPostLogPointTable, GroupEnum\n",
    "# from src.utils import get_logger, safe_call\n",
    "\n",
    "CLIENT_ID = os.getenv(\"CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"CLIENT_SECRET\")\n",
    "USERNAME = os.getenv(\"USERNAME\")\n",
    "PASSWORD = os.getenv(\"PASSWORD\")\n",
    "USER_AGENT = os.getenv(\"USER_AGENT\")\n",
    "RATELIMIT = int(os.getenv(\"RATELIMIT\", 5))\n",
    "\n",
    "logger = get_logger(\"REDDIT-BOT\")\n",
    "\n",
    "\n",
    "class RedditBot:\n",
    "    \"\"\"\n",
    "    Wrapper for the Reddit bot.\n",
    "\n",
    "    It contains the following methods:\n",
    "    - get_batch_of_posts: Selects a batch of posts for the experiment\n",
    "    - group_posts: Groups a list of posts into treatment and control groups\n",
    "    - add_posts_to_db: Adds a list of posts to the database\n",
    "    - add_log_points: Adds a list of log points to the database\n",
    "    - get_stored_posts: Fetches posts from the database given a date filter\n",
    "    - get_posts: Fetches posts from the Reddit API given a list of ids\n",
    "    \"\"\"\n",
    "\n",
    "    reddit: praw.Reddit\n",
    "    url: str = \"https://www.reddit.com\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        client_id: str = CLIENT_ID,\n",
    "        client_secret: str = CLIENT_SECRET,\n",
    "        username: str = USERNAME,\n",
    "        password: str = PASSWORD,\n",
    "        user_agent: str = USER_AGENT,\n",
    "        ratelimit: int = RATELIMIT,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Authenticates the bot and initializes the Reddit instance.\n",
    "        \"\"\"\n",
    "        assert isinstance(client_id, str)\n",
    "        assert isinstance(client_secret, str)\n",
    "        assert isinstance(username, str)\n",
    "        assert isinstance(password, str)\n",
    "        assert isinstance(user_agent, str)\n",
    "        assert isinstance(ratelimit, int)\n",
    "\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret,\n",
    "            username=username,\n",
    "            password=password,\n",
    "            user_agent=user_agent,\n",
    "            ratelimit=ratelimit,\n",
    "        )\n",
    "\n",
    "    def get_batch_of_posts(\n",
    "        self,\n",
    "        subreddit: str = \"all\",\n",
    "        score: int = 1,\n",
    "        num_comments: int = 1,\n",
    "        batch_size: int = 64,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Selects a batch of posts with at most 'score' number of upvotes and\n",
    "        'num_comments' number of comments in the given subreddit.\n",
    "\n",
    "        NOTE: batch_size is an upper bound on the number of posts returned.\n",
    "        \"\"\"\n",
    "\n",
    "        posts = [\n",
    "            post\n",
    "            for post in self.reddit.subreddit(subreddit).new(limit=batch_size)\n",
    "            if post.score <= score and post.num_comments <= num_comments\n",
    "        ]\n",
    "\n",
    "        return posts\n",
    "\n",
    "    @staticmethod\n",
    "    def group_posts(\n",
    "        posts: List[praw.models.Submission],\n",
    "    ) -> Tuple[List[praw.models.Submission], List[praw.models.Submission]]:\n",
    "        \"\"\"\n",
    "        Assigns posts into treatment and control groups.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_id = str(uuid4())\n",
    "        random.shuffle(posts)\n",
    "        if len(posts) % 2 != 0:\n",
    "            posts.pop()  # Drop a random post to make the list even\n",
    "\n",
    "        middle = len(posts) // 2\n",
    "        treatment_posts = posts[:middle]\n",
    "        control_posts = posts[middle:]\n",
    "\n",
    "        for post in treatment_posts:\n",
    "            post.upvote()\n",
    "            post.group = GroupEnum.TREATMENT\n",
    "            post.batch_id = batch_id\n",
    "\n",
    "        for post in control_posts:\n",
    "            post.group = GroupEnum.CONTROL\n",
    "            post.batch_id = batch_id\n",
    "\n",
    "        return treatment_posts, control_posts\n",
    "\n",
    "    @staticmethod\n",
    "    def add_posts_to_db(\n",
    "        posts: List[praw.models.Submission],\n",
    "        backup: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "        prepared_posts = []\n",
    "\n",
    "        def _prepare_post(post: praw.models.Submission) -> Dict:\n",
    "            return dict(\n",
    "                id=post.id,\n",
    "                batch_id=post.batch_id,\n",
    "                group=post.group,\n",
    "                subreddit=str(post.subreddit),\n",
    "                title=post.title,\n",
    "                creation_date=post.created_utc,\n",
    "            )\n",
    "\n",
    "        for post in posts:\n",
    "            prepared_post = safe_call(\n",
    "                _prepare_post,\n",
    "                args=[post],\n",
    "                exception=Exception,\n",
    "                raise_on_failure=False,\n",
    "            )\n",
    "            if prepared_post is not None:\n",
    "                prepared_posts.append(prepared_post)\n",
    "\n",
    "        if backup:\n",
    "            today = datetime.today().date().isoformat()\n",
    "            with open(f\"backup/REDDITBOT_{today}_{str(uuid4())}.json\", \"w\") as f:\n",
    "                json.dump(prepared_posts, f, indent=4, default=str)\n",
    "\n",
    "        db = DBFactory()()\n",
    "\n",
    "        parsed_posts = [RedditPostTable(**post) for post in prepared_posts]\n",
    "        db.add(parsed_posts)\n",
    "        logger.info(f\"Added {len(parsed_posts)} posts to the database\")\n",
    "\n",
    "        RedditBot.add_log_points(posts, backup=backup)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_log_points(\n",
    "        posts: List[praw.models.Submission], backup: bool = False\n",
    "    ) -> None:\n",
    "\n",
    "        prepared_posts = []\n",
    "        stale_posts = []\n",
    "\n",
    "        def _prepare_post(post: praw.models.Submission) -> Dict:\n",
    "            return dict(\n",
    "                id=post.id,\n",
    "                score=post.score,\n",
    "                num_comments=post.num_comments,\n",
    "                date=datetime.now(),\n",
    "            )\n",
    "\n",
    "        for post in posts:\n",
    "            prepared_post = safe_call(\n",
    "                _prepare_post,\n",
    "                args=[post],\n",
    "                exception=Exception,\n",
    "                raise_on_failure=False,\n",
    "            )\n",
    "            if prepared_post is not None:\n",
    "                prepared_posts.append(prepared_post)\n",
    "            else:\n",
    "                stale_posts.append(post)\n",
    "\n",
    "        if backup:\n",
    "            today = datetime.today().date().isoformat()\n",
    "            with open(f\"backup/REDDITBOT_{today}_{str(uuid4())}.json\", \"w\") as f:\n",
    "                json.dump(prepared_posts, f, indent=4, default=str)\n",
    "\n",
    "        db = DBFactory()()\n",
    "\n",
    "        parsed_posts = [RedditPostLogPointTable(**post) for post in prepared_posts]\n",
    "        db.add(parsed_posts)\n",
    "        logger.info(f\"Added {len(parsed_posts)} log points to database\")\n",
    "\n",
    "        for post in stale_posts:\n",
    "            old_instance = db.get(RedditPostTable, id=post.id)\n",
    "            if old_instance is not None:\n",
    "                old_instance.active = False\n",
    "                db.add([old_instance])\n",
    "\n",
    "        logger.info(f\"Marked {len(stale_posts)} stale posts\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_stored_posts(max_age: int = 8) -> List[RedditPostTable]:\n",
    "        db = DBFactory()()\n",
    "\n",
    "        with db.session() as session:\n",
    "            posts = (\n",
    "                session.query(RedditPostTable)\n",
    "                .filter(\n",
    "                    RedditPostTable.creation_date\n",
    "                    >= (datetime.now() - timedelta(days=max_age))\n",
    "                )\n",
    "                .filter(RedditPostTable.active)\n",
    "                .all()\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Fetched {len(posts)} active posts from the database\")\n",
    "\n",
    "            return posts\n",
    "\n",
    "    def _submission_wrapper(self, *args, **kwargs) -> Optional[praw.models.Submission]:\n",
    "        try:\n",
    "            return self.reddit.submission(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                f\"{e}\\n{traceback.format_exc()}\\nargs: {args}\\nkwargs: {kwargs}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    def get_posts(\n",
    "        self, ids: List[str], threads: int = 4\n",
    "    ) -> List[praw.models.Submission]:\n",
    "        with ThreadPool(threads) as pool:\n",
    "            posts = pool.map(self._submission_wrapper, ids)\n",
    "\n",
    "        posts = [post for post in posts if post is not None]\n",
    "\n",
    "        return posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3d40ed",
   "metadata": {},
   "source": [
    "### One-time entrypoint - `setup.py`\n",
    "The `setup` function simply establishes a connection to- and possibly creates the database, after which it drops any existing tables and creates new ones afresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32395a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup.py\n",
    "\n",
    "# Local imports suppressed in notebook cells since the objects are already available in scope.\n",
    "# from src.database_models import RedditPostTable, RedditPostLogPointTable  # NOQA : F401\n",
    "# from src.database import DBFactory\n",
    "\n",
    "\n",
    "def setup():\n",
    "    db = DBFactory()()\n",
    "    db.drop_tables()\n",
    "    db.create_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f43d3e",
   "metadata": {},
   "source": [
    "### Main entrypoint - `main.py`\n",
    "With everything set up, the `main` function defines a simple routine for monitoring the stats of previously fetched posts as well as fetching a batch of newly created posts. This is the function that is deployed to run periodically. Check [the GitHub repo](https://github.com/NValsted/RDS-Project-2022-1) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af34d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "# Local imports suppressed in notebook cells since the objects are already available in scope.\n",
    "# from src import RedditBot\n",
    "# from src.utils import safe_call\n",
    "\n",
    "\n",
    "def main():\n",
    "    bot = RedditBot()\n",
    "\n",
    "    # Update old Posts\n",
    "    posts = bot.get_stored_posts()\n",
    "    ids = {post.id for post in posts}\n",
    "\n",
    "    posts = bot.get_posts(ids)\n",
    "    bot.add_log_points(posts)\n",
    "\n",
    "    # New posts\n",
    "    treatment, control = safe_call(\n",
    "        func=lambda: bot.group_posts(bot.get_batch_of_posts())\n",
    "    )\n",
    "    bot.add_posts_to_db(treatment)\n",
    "    bot.add_posts_to_db(control)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27102d",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "At this point, over 20000 posts have been fetched and monitored over the course of 7 days each, which has resulted in around 1.5 million log points. This section marks the start of an analysis of the resulting data.\n",
    "\n",
    "A snapshot of the database is available at https://ituniversity-my.sharepoint.com/:u:/g/personal/nicv_itu_dk/ESyJlN06ZbJEsYJSpBH6zSEB8IKTH5iKjAMcHIjumsXfIQ?e=YGatIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f647ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df1c96",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d19ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_post_df = pd.read_sql_table(RedditPostTable.__tablename__, DBFactory.engine_url)\n",
    "reddit_post_log_point_df = pd.read_sql_table(RedditPostLogPointTable.__tablename__, DBFactory.engine_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94eecc",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Before commencing with the analysis, a little preprocessing is beneficial, e.g. due to the fact that certain posts are marked inactive since they have been deleted or otherwise made unreachable, which has unbalanced the dataset slightly.\n",
    "\n",
    "The preprocessing steps are the following (Which are intertwined in practice for convenience):\n",
    "- Join post info and log points (`RedditPostTable` and `RedditPostLogPointTable`)\n",
    "- Balance dataset. \n",
    "    - For each inactive post, identify a post from the conjugate group with the same `batch_id` and filter away both.\n",
    "    - Filter away any post that has not been monitored for at least 7 days (i.e. younger than 7 days or marked inactive before the 7 day mark).\n",
    "- Create derived columns: `age` and `saturation`.\n",
    "- Unbias treatment group by subtracting 1 from the score\n",
    "- For each of the two groups, create dataframes containing only the latest log point for a post.\n",
    "\n",
    "These steps will be described further in the relevant cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f326aeda",
   "metadata": {},
   "source": [
    "### Join dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6fea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = pd.merge(reddit_post_df, reddit_post_log_point_df, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e67e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b9e7db",
   "metadata": {},
   "source": [
    "### Derived columns p0 - age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb0106",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined[\"age\"] = joined[\"date\"] - joined[\"creation_date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55acf5",
   "metadata": {},
   "source": [
    "### Unbias TREATMENT group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88cfa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined[joined[\"group\"] == GroupEnum.TREATMENT.value][\"score\"] -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d0a1b4",
   "metadata": {},
   "source": [
    "### Latest posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484b3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_latest = joined.sort_values([\"date\"], ascending=False).groupby(by=\"id\", as_index=False).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f36d7d",
   "metadata": {},
   "source": [
    "### Balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034c6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_drop = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa300b",
   "metadata": {},
   "source": [
    "Filter posts younger than 7 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6443046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in joined_latest[joined_latest[\"age\"] < timedelta(days=7)].iterrows():\n",
    "    ids_to_drop.add(str(row[\"id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6644a7f",
   "metadata": {},
   "source": [
    "Balance pairs within batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf365747",
   "metadata": {},
   "outputs": [],
   "source": [
    "inactive_posts = joined_latest[joined_latest[\"active\"] == False]\n",
    "\n",
    "CONJUGATE_MAP = dict(\n",
    "    CONTROL=\"TREATMENT\",\n",
    "    TREATMENT=\"CONTROL\",\n",
    ")\n",
    "\n",
    "for group in inactive_posts.groupby(by=\"batch_id\", as_index=False):\n",
    "    group_key, group_df = group\n",
    "\n",
    "    for source, conjugate in CONJUGATE_MAP.items():\n",
    "        num_to_drop = group_df[group_df[\"group\"] == source].shape[0]\n",
    "\n",
    "        if num_to_drop > 0:\n",
    "            to_drop = joined_latest[\n",
    "                (joined_latest[\"batch_id\"] == group_key)\n",
    "                & (joined_latest[\"group\"] == conjugate)\n",
    "                & ~(joined_latest[\"id\"].isin(set(group_df[\"id\"])))\n",
    "            ].sample(n=num_to_drop)\n",
    "\n",
    "            ids_to_drop.add(str(to_drop[\"id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1663f278",
   "metadata": {},
   "source": [
    "Apply filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dropping {len(ids_to_drop)} post(s)\")\n",
    "\n",
    "joined_latest = joined_latest[~(joined_latest[\"id\"].isin(ids_to_drop))]\n",
    "# joined = joined[~(joined[\"id\"].isin(ids_to_drop))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1ecf4",
   "metadata": {},
   "source": [
    "### Derived columns p1 - saturation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac30f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined[\"score_saturation\"] = joined[\"score\"] / joined.groupby(\"id\")[\"score\"].transform(np.max)\n",
    "joined[\"num_comments_saturation\"] = joined[\"num_comments\"] / joined.groupby(\"id\")[\"num_comments\"].transform(np.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ede8aa",
   "metadata": {},
   "source": [
    "### Overview of resulting dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4771b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38ac38",
   "metadata": {},
   "source": [
    "### Split dataframe in groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e906a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "control, treatment = (joined[joined[\"group\"] == group.value] for group in (GroupEnum.CONTROL, GroupEnum.TREATMENT))\n",
    "control_latest, treatment_latest = (\n",
    "    joined_latest[joined_latest[\"group\"] == group.value] for group in (GroupEnum.CONTROL, GroupEnum.TREATMENT)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0545e336",
   "metadata": {},
   "source": [
    "## Distribution similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import io as pio, graph_objects as go, express as px\n",
    "from plotly.subplots import make_subplots\n",
    "pio.renderers.default = \"iframe\"\n",
    "# Jupyter notebooks don't handle plotly figures well.\n",
    "# Therefore, iframes and %%capture are used to save the resulting html files to disk instead.\n",
    "# These should appear in the iframe_figures directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56afc4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    shared_yaxes=True,\n",
    "    subplot_titles=(\"Score distribution\", \"Number of comments distribution\"),\n",
    "    horizontal_spacing=0.05,\n",
    ")\n",
    "\n",
    "for i in (1, 2):\n",
    "    for j, attr in ((1, \"score\"), (2, \"num_comments\")):\n",
    "        for df, color in ((control_latest, \"#4969AA\"), (treatment_latest, \"#C90D0D\")):\n",
    "            fig.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=df[attr],\n",
    "                    histnorm=\"percent\",\n",
    "                    name=df[\"group\"].min(),\n",
    "                    xbins=dict(\n",
    "                        start=df[attr].min(),\n",
    "                        end=df[attr].max(),\n",
    "                        size=0.5\n",
    "                    ),\n",
    "                    marker_color=color,\n",
    "                    opacity=0.75,\n",
    "                    showlegend=i == 1 and j == 1\n",
    "                ),\n",
    "                row=i,\n",
    "                col=j,\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            yaxis_title_text=\"Percent\",\n",
    "            bargap=0.2,\n",
    "            bargroupgap=0.1\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(type=\"log\", row=1, col=1)\n",
    "fig.update_xaxes(type=\"log\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Score (log)\", row=2, col=1, type=\"log\")\n",
    "fig.update_xaxes(title_text=\"Number of comments (log)\", row=2, col=2, type=\"log\")\n",
    "\n",
    "fig.update_yaxes(title_text=\"Percent (log)\", type=\"log\", row=2, col=1)\n",
    "fig.update_yaxes(type=\"log\", row=2, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b615d28c",
   "metadata": {},
   "source": [
    "### Kolmogorov-Smirnov test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dafe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.kstest(control_latest[\"score\"], treatment_latest[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261bf17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.kstest(control_latest[\"num_comments\"], treatment_latest[\"num_comments\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc95ae8",
   "metadata": {},
   "source": [
    "Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc1c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_ks_test_score = [stats.kstest(*(df[\"score\"].map(lambda x: i * round(x / i)) for df in (control_latest, treatment_latest))).pvalue for i in tqdm(range(1, 10))]\n",
    "binned_ks_test_num_comments = [stats.kstest(*(df[\"num_comments\"].map(lambda x: i * round(x / i)) for df in (control_latest, treatment_latest))).pvalue for i in tqdm(range(1, 10))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    subplot_titles=(\"KS-test binned score distributions\", \"KS-test binned number of comments distributions\"),\n",
    ")\n",
    "for i, lst in enumerate((binned_ks_test_score, binned_ks_test_num_comments)):\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, len(lst) + 1)), y=lst), row=i+1, col=1)\n",
    "fig.update_yaxes(type=\"log\", row=\"all\", col=\"all\")\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb9cef",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c963f1",
   "metadata": {},
   "source": [
    "#### Quantile statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21caaffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in (\"score\", \"num_comments\"):\n",
    "    for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest)):\n",
    "        print(f\"{name}: {attr}\")\n",
    "        print(df[attr].describe())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bcd908",
   "metadata": {},
   "source": [
    "#### Skewness and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b5627",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in (\"skew\", \"kurtosis\"):\n",
    "    print(f\"METRIC: {metric}\")\n",
    "    print(\n",
    "        \"\\n\".join(\n",
    "            (\n",
    "                f\"{name} {attr} {metric}: {getattr(df[attr], metric)()}\"\n",
    "                for attr in (\"score\", \"num_comments\")\n",
    "                for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf6c0a",
   "metadata": {},
   "source": [
    "#### (score, num_comments)-correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447d7e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest)):\n",
    "    print(f\"{name}:\")\n",
    "    print(df[\"score\"].corr(df[\"num_comments\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8588a",
   "metadata": {},
   "source": [
    "#### Filter extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top(df: pd.DataFrame, attr: str, qt: float = 0.95):\n",
    "    return df[df[attr] <= df[attr].quantile(qt)][attr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4d0f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in (\"score\", \"num_comments\"):\n",
    "    for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest)):\n",
    "        print(f\"{name}: {attr}\")\n",
    "        print(df[df[attr] <= df[attr].quantile(0.95)][attr].describe())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in (\"skew\", \"kurtosis\"):\n",
    "    print(f\"METRIC: {metric}\")\n",
    "    print(\n",
    "        \"\\n\".join(\n",
    "            (\n",
    "                f\"{name} {attr} {metric}: {getattr(df[df[attr] <= df[attr].quantile(0.95)][attr], metric)()}\"\n",
    "                for attr in (\"score\", \"num_comments\")\n",
    "                for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a1e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in ((\"CONTROL\", control_latest), (\"TREATMENT\", treatment_latest)):\n",
    "    print(f\"{name}:\")\n",
    "    print(filter_top(df, \"score\").corr(filter_top(df, \"num_comments\")))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa436805",
   "metadata": {},
   "source": [
    "## Temporal similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "px.scatter(joined, x=\"age\", y=\"score_saturation\", color=\"group\", opacity=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ff875",
   "metadata": {},
   "outputs": [],
   "source": [
    "control[\"score_saturation\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7218cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment[\"score_saturation\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b47481a",
   "metadata": {},
   "source": [
    "# Le conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d96ffa",
   "metadata": {},
   "source": [
    "In conclusion: From the KS-tests we deduce that the probability of the samples from the two distribution pairs - (TREATMENT_score, CONTROL_score) and (TREATMENT_num_comments, CONTROL_num_comments) - originating from the respective same distribution are both significantly less than 5% (p-value < 0.05). Investigating descriptive statistics of these distributions leads us to believe that the popularity of a post in terms of its score is indeed higher for posts in the treatment group. However, the opposite is true for the number of comments. Initially, we had hypothesized that score and number of comments were correlated, which does not seem to be the case, and this does tie together with the aforementioned uncorrelated effectiveness of the treatment on score and comments. However, a word of caution: The distributions are severely heavy-tailed as indicated by the kurtosis measures, and thus the extreme sample values can easily impact the results significantly, given the fairly limited size of the sample. This is demonstrated by filtering away the top 5% of the data from each distribution, yielding noticeably different perceived relationships between the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab1921f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
